{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BERT_Sentiment_Analysis_Pytorch.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyNTf8T40XePZj5HuC00fEBr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0b3349765b17428bb79608dadb1386d5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_75346e6d42d94e6e850a4e8ad503588f","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_88256d9a9f964ad492cf040393e9ce20","IPY_MODEL_90275722015c45adbc16ffa87f8e0990"]}},"75346e6d42d94e6e850a4e8ad503588f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"88256d9a9f964ad492cf040393e9ce20":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7ea30b5fc2f343c4ac3a0c894c8852e0","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3a26853122854d35b24c3fcbaf3ecdd3"}},"90275722015c45adbc16ffa87f8e0990":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_28368ac3b43b4afaac767910be8084b6","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 232k/232k [00:00&lt;00:00, 308kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_70ee573481dc420c803c5a67e52b2a10"}},"7ea30b5fc2f343c4ac3a0c894c8852e0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"3a26853122854d35b24c3fcbaf3ecdd3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"28368ac3b43b4afaac767910be8084b6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"70ee573481dc420c803c5a67e52b2a10":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"de574041486a4047bffb2ad6d7b11066":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_9358b68c78bc4835b66d902ef46ed521","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_67b9a1a344564a16aee1222b04e6903b","IPY_MODEL_09b061fc707b45408ca3818910e4a548"]}},"9358b68c78bc4835b66d902ef46ed521":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"67b9a1a344564a16aee1222b04e6903b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_0df254e16bd94a4294658d250d0c588e","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":433,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":433,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c484e8d4995e47f2aa77c923e2eeea5e"}},"09b061fc707b45408ca3818910e4a548":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4f1f33a9135b4938948fc6d8acc21ed5","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 433/433 [00:00&lt;00:00, 967B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5679ed8ad84f47cfb169058eb8406714"}},"0df254e16bd94a4294658d250d0c588e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"c484e8d4995e47f2aa77c923e2eeea5e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4f1f33a9135b4938948fc6d8acc21ed5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5679ed8ad84f47cfb169058eb8406714":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"82821b2396e04fb7b0bd7d07a27aa265":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_726562a64a3c49d9bc91c672bd4c0338","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_b809eb851dda487995bb586f0dc78548","IPY_MODEL_3d6c5333179845ec94722e13e6b6f038"]}},"726562a64a3c49d9bc91c672bd4c0338":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b809eb851dda487995bb586f0dc78548":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_fd305f6954bc40a692be6c7db90abef9","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":440473133,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":440473133,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_21337a6168744fecb1dbea442798067d"}},"3d6c5333179845ec94722e13e6b6f038":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1d027baa1ea7484980811b0446892fbe","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 440M/440M [00:07&lt;00:00, 56.3MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2b3371751d4e4276b781e78b4b726a73"}},"fd305f6954bc40a692be6c7db90abef9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"21337a6168744fecb1dbea442798067d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1d027baa1ea7484980811b0446892fbe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2b3371751d4e4276b781e78b4b726a73":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1afdbf82790e43c2837438f80875c78a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_e2851b9779004738b9fad09a4eb837ce","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_33b4c70c6931469d882be4280f135184","IPY_MODEL_72129be4fe044434adbfff68bcf8ebff"]}},"e2851b9779004738b9fad09a4eb837ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"33b4c70c6931469d882be4280f135184":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_9fa4ea8eb0af40b2bd53172bdde0c47b","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":10,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":10,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0a25fcf07bc14bfdb693b3eaa6a0a62c"}},"72129be4fe044434adbfff68bcf8ebff":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6fc02aa420404b9ea4f5cc220023f014","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 10/10 [06:00&lt;00:00, 36.01s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8cd5b14d63c04168b42ba0ab856f8f79"}},"9fa4ea8eb0af40b2bd53172bdde0c47b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0a25fcf07bc14bfdb693b3eaa6a0a62c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6fc02aa420404b9ea4f5cc220023f014":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8cd5b14d63c04168b42ba0ab856f8f79":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bf0b865efaa04b658bb4328b3f480af2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_8ec1e48ac84e4aa79c8b24023de6853d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_7133d9e9934f4af9a870fa15966e6ce7","IPY_MODEL_84faaf1372cb4934bb98e856477af8a7"]}},"8ec1e48ac84e4aa79c8b24023de6853d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7133d9e9934f4af9a870fa15966e6ce7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_617c8ad2c851437c9cda21736ede59f8","_dom_classes":[],"description":"Epoch 1: 100%","_model_name":"FloatProgressModel","bar_style":"","max":40,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":40,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0e81cfd1116b4be08de0152a9c7981e0"}},"84faaf1372cb4934bb98e856477af8a7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e69fbb5667e340f3bb5de4e581269e5c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 40/40 [00:30&lt;00:00,  1.61it/s, training loss=0.226]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_067fb02baeed4072aa6acb085431b42c"}},"617c8ad2c851437c9cda21736ede59f8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0e81cfd1116b4be08de0152a9c7981e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e69fbb5667e340f3bb5de4e581269e5c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"067fb02baeed4072aa6acb085431b42c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1e2c6fa236ad48429c3922cf6fb3147c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_7239a55baa1c465f9e7af21c45910310","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_045f532970f84314be5d2b0bfc2f8286","IPY_MODEL_26b5a48600214bd4aa51410cc483df44"]}},"7239a55baa1c465f9e7af21c45910310":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"045f532970f84314be5d2b0bfc2f8286":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_9637e0dc8bde4a7ca1fc86c075ab2c1b","_dom_classes":[],"description":"Epoch 2: 100%","_model_name":"FloatProgressModel","bar_style":"","max":40,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":40,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_850b5017aa1940da80c215c7d90b082f"}},"26b5a48600214bd4aa51410cc483df44":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_83aee442e18c4c74938d267788e8a9dc","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 40/40 [00:30&lt;00:00,  1.61it/s, training loss=0.347]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_71773ba203d042799eea8a183fa0f17a"}},"9637e0dc8bde4a7ca1fc86c075ab2c1b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"850b5017aa1940da80c215c7d90b082f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"83aee442e18c4c74938d267788e8a9dc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"71773ba203d042799eea8a183fa0f17a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"17d3ad976b094819832113c22445e33c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4c22846f86e74c0ca1f834eef5a67358","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_57024425657c46a7a5f72038da199960","IPY_MODEL_e3d6ff1a080344aea6a8a9b3e7f0382a"]}},"4c22846f86e74c0ca1f834eef5a67358":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"57024425657c46a7a5f72038da199960":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_fe675e807b5d458281fdb40717ebf794","_dom_classes":[],"description":"Epoch 3: 100%","_model_name":"FloatProgressModel","bar_style":"","max":40,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":40,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d92412b129fc4aebb0af011b5fe4185e"}},"e3d6ff1a080344aea6a8a9b3e7f0382a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4753ee97ecc54f57acb3dc6415d1ebf9","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 40/40 [00:30&lt;00:00,  1.61it/s, training loss=0.144]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e3e0fa7bfd374ff0836d06b9643b5c8c"}},"fe675e807b5d458281fdb40717ebf794":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"d92412b129fc4aebb0af011b5fe4185e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4753ee97ecc54f57acb3dc6415d1ebf9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e3e0fa7bfd374ff0836d06b9643b5c8c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"eb579ce308014c9d876ebf2fab20dec0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_f502d1b58dd9443393c3b361f915ed5e","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_0dad2f4711104690990bea2a8633adf8","IPY_MODEL_67cf3149c4044dc0a4966ce3b1cf2549"]}},"f502d1b58dd9443393c3b361f915ed5e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0dad2f4711104690990bea2a8633adf8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7b97d45cd3104fd181fd274249fc7862","_dom_classes":[],"description":"Epoch 4: 100%","_model_name":"FloatProgressModel","bar_style":"","max":40,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":40,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_687194a2d19a48638e4424f9fe6d0f7b"}},"67cf3149c4044dc0a4966ce3b1cf2549":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b4c807c2e4e24683a0333236c1a6ec13","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 40/40 [00:30&lt;00:00,  1.61it/s, training loss=0.239]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f2a61c7075b246848ee79b58dbbabe4b"}},"7b97d45cd3104fd181fd274249fc7862":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"687194a2d19a48638e4424f9fe6d0f7b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b4c807c2e4e24683a0333236c1a6ec13":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f2a61c7075b246848ee79b58dbbabe4b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a2d5fffdb8904ec48eb2f7f49420e4ce":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c3634aa125f7416fa4bb6f3b302dc4fd","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d2a2b5e583d541e4ae7d9b198054e83a","IPY_MODEL_040ede06e9ad404ea811f2c5c676918c"]}},"c3634aa125f7416fa4bb6f3b302dc4fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d2a2b5e583d541e4ae7d9b198054e83a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_dad8bfe53d3e4d0a91b9bc8077bd53d5","_dom_classes":[],"description":"Epoch 5: 100%","_model_name":"FloatProgressModel","bar_style":"","max":40,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":40,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_858800004a31480eb4a2f029fe39a5ef"}},"040ede06e9ad404ea811f2c5c676918c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_68d9f656d5524afc9eff8ee451f85f6b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 40/40 [00:30&lt;00:00,  1.60it/s, training loss=0.074]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_36962c374b7e4d1e9717a3b6d4cb34d5"}},"dad8bfe53d3e4d0a91b9bc8077bd53d5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"858800004a31480eb4a2f029fe39a5ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"68d9f656d5524afc9eff8ee451f85f6b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"36962c374b7e4d1e9717a3b6d4cb34d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ac4c5b6bec0743899cc7d452435acbf4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_0143e5a95da248c281952f377cb4302a","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_4b343d916a7448e1b5b57bdbaeff51cb","IPY_MODEL_70c6110a3d8449ad8f5fd6674f30da79"]}},"0143e5a95da248c281952f377cb4302a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4b343d916a7448e1b5b57bdbaeff51cb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_974f786bc2ec4021a36c37622a813ee0","_dom_classes":[],"description":"Epoch 6: 100%","_model_name":"FloatProgressModel","bar_style":"","max":40,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":40,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_97313c3713a04981b261dee02172b67f"}},"70c6110a3d8449ad8f5fd6674f30da79":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9991f80c940f46af9c7ac96e21ab26f8","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 40/40 [00:30&lt;00:00,  1.60it/s, training loss=0.119]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_299bbc871f6e4c7c9040b364c20b98f4"}},"974f786bc2ec4021a36c37622a813ee0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"97313c3713a04981b261dee02172b67f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9991f80c940f46af9c7ac96e21ab26f8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"299bbc871f6e4c7c9040b364c20b98f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bb04bf8df8504a0fa47a1cfe7f94cd87":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_7d05fad76f8c406c9fe134272a8529f6","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_49b0c690eeb94a6f848c7b57d8d04e68","IPY_MODEL_2a0d6955171c4b849b0b89f921b24fa5"]}},"7d05fad76f8c406c9fe134272a8529f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"49b0c690eeb94a6f848c7b57d8d04e68":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_48a6ec78a2aa4897b7be5d42b5b85d42","_dom_classes":[],"description":"Epoch 7: 100%","_model_name":"FloatProgressModel","bar_style":"","max":40,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":40,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e2d8f84dda204afaa4d83a6ef9677fe8"}},"2a0d6955171c4b849b0b89f921b24fa5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_c89f2f878f284a699deeddc2d7869c5d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 40/40 [00:30&lt;00:00,  1.60it/s, training loss=0.043]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_916884d6d5e74840b97003c61e1c05b3"}},"48a6ec78a2aa4897b7be5d42b5b85d42":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e2d8f84dda204afaa4d83a6ef9677fe8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c89f2f878f284a699deeddc2d7869c5d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"916884d6d5e74840b97003c61e1c05b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"793b914f535245a991866133ff4f3a9a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d731f44a83534a119f5e12eaa300ec48","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_3595f76b537c4ff997aa27919bd6419f","IPY_MODEL_ae3cfa0361624b09b25700d2bb151368"]}},"d731f44a83534a119f5e12eaa300ec48":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3595f76b537c4ff997aa27919bd6419f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_a9e27bb879be4226b12a830ee4d83263","_dom_classes":[],"description":"Epoch 8: 100%","_model_name":"FloatProgressModel","bar_style":"","max":40,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":40,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6c5623a6e1b844d9a0c0b65b82e1ffdd"}},"ae3cfa0361624b09b25700d2bb151368":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6e6e2568fed94e3dbfc72f37826b7a06","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 40/40 [00:30&lt;00:00,  1.60it/s, training loss=0.275]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_09d5e605016343688ada38b90de21f9d"}},"a9e27bb879be4226b12a830ee4d83263":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"6c5623a6e1b844d9a0c0b65b82e1ffdd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6e6e2568fed94e3dbfc72f37826b7a06":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"09d5e605016343688ada38b90de21f9d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ac85eedc899748f1bf3e181dc583a026":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_9271177d79904a158ca4f1cb043bb7db","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_16b6b8d2a7084918bbe8dc9dff118ea8","IPY_MODEL_3a5d821234294111a2d7f0b7864f87a8"]}},"9271177d79904a158ca4f1cb043bb7db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"16b6b8d2a7084918bbe8dc9dff118ea8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_60f328b20ea34b7684556d30069b6c4f","_dom_classes":[],"description":"Epoch 9: 100%","_model_name":"FloatProgressModel","bar_style":"","max":40,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":40,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7f80a09d760e4d5b9d5662b45da9e1fc"}},"3a5d821234294111a2d7f0b7864f87a8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_89d04c7fcfda400aab44787b0bfbb94e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 40/40 [00:30&lt;00:00,  1.61it/s, training loss=0.111]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1be5843f40d94132bf31b646376b4ab5"}},"60f328b20ea34b7684556d30069b6c4f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"7f80a09d760e4d5b9d5662b45da9e1fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"89d04c7fcfda400aab44787b0bfbb94e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"1be5843f40d94132bf31b646376b4ab5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1ccdb58e45b74058b443aa89b1a0e920":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_efa2b28238304c6f92ec14d89d3e71a8","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_b6fa04f82751418fb8654be0632490dd","IPY_MODEL_0f81abf07e074618bc8f1bf9eab964e8"]}},"efa2b28238304c6f92ec14d89d3e71a8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b6fa04f82751418fb8654be0632490dd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7753e8a45678470a958de2e47afe937b","_dom_classes":[],"description":"Epoch 10: 100%","_model_name":"FloatProgressModel","bar_style":"","max":40,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":40,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_10bd83627b2c4990a61f475c7f85a661"}},"0f81abf07e074618bc8f1bf9eab964e8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3467a1db3019490da992a3e332dc218c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 40/40 [00:30&lt;00:00,  1.61it/s, training loss=0.049]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e957da41330d4ab5acf7f0e3deeba804"}},"7753e8a45678470a958de2e47afe937b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"10bd83627b2c4990a61f475c7f85a661":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3467a1db3019490da992a3e332dc218c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e957da41330d4ab5acf7f0e3deeba804":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c6f250489d954d48808cddab6b325813":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_2b27d40c56de4dbfaabb8582f1a660be","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_5299bdcc17eb4b51953ecd2672d24989","IPY_MODEL_446eb9d25cd64664b51f74f0929e9d03"]}},"2b27d40c56de4dbfaabb8582f1a660be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5299bdcc17eb4b51953ecd2672d24989":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_39d985768c0f4787a748d2260024cf20","_dom_classes":[],"description":"  0%","_model_name":"FloatProgressModel","bar_style":"","max":10,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":0,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ff02ba3724974a2d8c5a6a2ec8d62b0c"}},"446eb9d25cd64664b51f74f0929e9d03":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_657dc1de8dfc47d2bc706d84429e001b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 0/10 [00:00&lt;?, ?it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ee39d31abfd8425faf426262bb188ebb"}},"39d985768c0f4787a748d2260024cf20":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ff02ba3724974a2d8c5a6a2ec8d62b0c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"657dc1de8dfc47d2bc706d84429e001b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ee39d31abfd8425faf426262bb188ebb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cfeddb91faf54274a7daee7a48636faf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_7a47cd32adb74061b4ef69d3a860850d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_5039179b211541cab48a49d287563b08","IPY_MODEL_dcfcc89092d84a86a3249ad7d8e75a0d"]}},"7a47cd32adb74061b4ef69d3a860850d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5039179b211541cab48a49d287563b08":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_df647f5947614092bc17630553bf541d","_dom_classes":[],"description":"Epoch 3:   0%","_model_name":"FloatProgressModel","bar_style":"","max":40,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":0,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_abf4e078c29b49b09930f9ee5754adba"}},"dcfcc89092d84a86a3249ad7d8e75a0d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_76b9a1234f1440bfb4831b8058813c2f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 0/40 [00:00&lt;?, ?it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b594307020f54b76a2f94b9945f5d14c"}},"df647f5947614092bc17630553bf541d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"abf4e078c29b49b09930f9ee5754adba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"76b9a1234f1440bfb4831b8058813c2f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b594307020f54b76a2f94b9945f5d14c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"73899fe9a41543f18afbe83fdbc7fe1f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c424479433184da18c6ec6449f1316bc","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f8e1d281baff460b99826b0205ea1de1","IPY_MODEL_e7ca93142ea04337b0e4f701f82c3e7c"]}},"c424479433184da18c6ec6449f1316bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f8e1d281baff460b99826b0205ea1de1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_ab14104299fd44bfbdd102b6e65dd205","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":40,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":40,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_472eb0903b484357b0034d7c4b10dff3"}},"e7ca93142ea04337b0e4f701f82c3e7c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3046241f362a444aa68b4f0063676e39","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 40/40 [00:00&lt;00:00, 482.28it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_51f6afe2c1b34d8999ce9da264a385fc"}},"ab14104299fd44bfbdd102b6e65dd205":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"472eb0903b484357b0034d7c4b10dff3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3046241f362a444aa68b4f0063676e39":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"51f6afe2c1b34d8999ce9da264a385fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"jxauZykPNpqc"},"source":["# Sentiment Analysis with Deep Learning using BERT"]},{"cell_type":"markdown","metadata":{"id":"-U5n_4phNygw"},"source":["### Project Outline"]},{"cell_type":"markdown","metadata":{"id":"Kg_Tk-j0NvX8"},"source":["**Task 1**: Introduction (this section)\n","\n","**Task 2**: Exploratory Data Analysis and Preprocessing\n","\n","**Task 3**: Training/Validation Split\n","\n","**Task 4**: Loading Tokenizer and Encoding our Data\n","\n","**Task 5**: Setting up BERT Pretrained Model\n","\n","**Task 6**: Creating Data Loaders\n","\n","**Task 7**: Setting Up Optimizer and Scheduler\n","\n","**Task 8**: Defining our Performance Metrics\n","\n","**Task 9**: Creating our Training Loop\n","\n","**Task 10**: Loading and Evaluating our Model"]},{"cell_type":"markdown","metadata":{"id":"IAEsTdZ0OCxP"},"source":["## Task 1: Introduction"]},{"cell_type":"markdown","metadata":{"id":"E-v72-z5N4oy"},"source":["### What is BERT\n","\n","BERT is a large-scale transformer-based Language Model that can be finetuned for a variety of tasks.\n","\n","For more information, the original paper can be found [here](https://arxiv.org/abs/1810.04805). \n","\n","[HuggingFace documentation](https://huggingface.co/transformers/model_doc/bert.html)\n","\n","[Bert documentation](https://characters.fandom.com/wiki/Bert_(Sesame_Street) ;)\n"]},{"cell_type":"markdown","metadata":{"id":"EsPKqgVcOMjy"},"source":["## Task 2: Exploratory Data Analysis and Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"c9cpqsaEOOLS"},"source":["We will use the SMILE Twitter dataset.\n","\n","_Wang, Bo; Tsakalidis, Adam; Liakata, Maria; Zubiaga, Arkaitz; Procter, Rob; Jensen, Eric (2016): SMILE Twitter Emotion dataset. figshare. Dataset. https://doi.org/10.6084/m9.figshare.3187909.v2_"]},{"cell_type":"code","metadata":{"id":"AjxF3SE7Mej-","executionInfo":{"status":"ok","timestamp":1602682769579,"user_tz":-420,"elapsed":4779,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}}},"source":["import os\n","import torch\n","import pandas as pd\n","from tqdm.notebook import tqdm"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xrp3stqHOvz7","executionInfo":{"status":"ok","timestamp":1602682794857,"user_tz":-420,"elapsed":30048,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"55b51d03-ac0c-4591-cfa9-0124f265caf3","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# connect Google Colab with Google Drive to read file(s) from Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AWXkkaavql2d","executionInfo":{"status":"ok","timestamp":1602682794857,"user_tz":-420,"elapsed":30040,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}}},"source":["directory_path = '/content/drive/My Drive/CIT/SEMESTER3/I.INTEL/SUB2_Coursera_BERT_SentimentAnalysis'\n","file_name = 'smileannotationsfinal.csv'"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"iK3YgJkfPDq0","executionInfo":{"status":"ok","timestamp":1602682797254,"user_tz":-420,"elapsed":32432,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}}},"source":["# loading the data set\n","df = pd.read_csv(os.path.join(directory_path, file_name), names=['id','text','category'])\n","df.set_index('id', inplace=True)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"vH3pk_GTPVvt","executionInfo":{"status":"ok","timestamp":1602682797255,"user_tz":-420,"elapsed":32429,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"bff30371-adda-4cc5-b7e0-3fc2aa40752d","colab":{"base_uri":"https://localhost:8080/","height":228}},"source":["# having a quick look for the dataset with first 5 rows\n","df.head()"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>category</th>\n","    </tr>\n","    <tr>\n","      <th>id</th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>611857364396965889</th>\n","      <td>@aandraous @britishmuseum @AndrewsAntonio Merc...</td>\n","      <td>nocode</td>\n","    </tr>\n","    <tr>\n","      <th>614484565059596288</th>\n","      <td>Dorian Gray with Rainbow Scarf #LoveWins (from...</td>\n","      <td>happy</td>\n","    </tr>\n","    <tr>\n","      <th>614746522043973632</th>\n","      <td>@SelectShowcase @Tate_StIves ... Replace with ...</td>\n","      <td>happy</td>\n","    </tr>\n","    <tr>\n","      <th>614877582664835073</th>\n","      <td>@Sofabsports thank you for following me back. ...</td>\n","      <td>happy</td>\n","    </tr>\n","    <tr>\n","      <th>611932373039644672</th>\n","      <td>@britishmuseum @TudorHistory What a beautiful ...</td>\n","      <td>happy</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                                 text category\n","id                                                                            \n","611857364396965889  @aandraous @britishmuseum @AndrewsAntonio Merc...   nocode\n","614484565059596288  Dorian Gray with Rainbow Scarf #LoveWins (from...    happy\n","614746522043973632  @SelectShowcase @Tate_StIves ... Replace with ...    happy\n","614877582664835073  @Sofabsports thank you for following me back. ...    happy\n","611932373039644672  @britishmuseum @TudorHistory What a beautiful ...    happy"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"JGJyi_SYPauo","executionInfo":{"status":"ok","timestamp":1602682797255,"user_tz":-420,"elapsed":32421,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"9d2c0254-a040-469c-e262-1b2970840ade","colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["# printing out one sample text\n","df.text.iloc[0]"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'@aandraous @britishmuseum @AndrewsAntonio Merci pour le partage! @openwinemap'"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"GzqwQ4pjPs12","executionInfo":{"status":"ok","timestamp":1602682797256,"user_tz":-420,"elapsed":32413,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"fea19cb7-14ea-47a0-9ef3-a3497f4a553e","colab":{"base_uri":"https://localhost:8080/","height":259}},"source":["# examining the counts for each of the values in category\n","df.category.value_counts()"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["nocode               1572\n","happy                1137\n","not-relevant          214\n","angry                  57\n","surprise               35\n","sad                    32\n","happy|surprise         11\n","happy|sad               9\n","disgust|angry           7\n","disgust                 6\n","sad|angry               2\n","sad|disgust             2\n","sad|disgust|angry       1\n","Name: category, dtype: int64"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"TFMgNg2NP9o6","executionInfo":{"status":"ok","timestamp":1602682797256,"user_tz":-420,"elapsed":32405,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}}},"source":["# removing observations having more than 2 feelings, \n","# i.e. observations having character \"|\" in the category\n","df = df[~df.category.str.contains('\\|')]"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"QuUwFATBQVnN","executionInfo":{"status":"ok","timestamp":1602682797258,"user_tz":-420,"elapsed":32401,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}}},"source":["# removing observations having no feeling, i.e. 'nocode'\n","df = df[df.category != 'nocode']"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"rvHSpq1hQh38","executionInfo":{"status":"ok","timestamp":1602682797258,"user_tz":-420,"elapsed":32396,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"83345724-00ea-4b64-87cc-4ac4d2719f43","colab":{"base_uri":"https://localhost:8080/","height":138}},"source":["# examining again the counts for each of the values in category\n","df.category.value_counts()"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["happy           1137\n","not-relevant     214\n","angry             57\n","surprise          35\n","sad               32\n","disgust            6\n","Name: category, dtype: int64"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"_2KaJFy2RsII","executionInfo":{"status":"ok","timestamp":1602682797259,"user_tz":-420,"elapsed":32389,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"84931fa8-3670-4160-85bf-b0b7a89b9108","colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["# building a dictionary containing keys as feelings in category and\n","# values being 0,1,2,... accordingly\n","\n","# firstly creating a list containing all of the keys/values\n","possible_labels = df.category.unique()\n","possible_labels"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['happy', 'not-relevant', 'angry', 'disgust', 'sad', 'surprise'],\n","      dtype=object)"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"UPmwgqSmSFJE","executionInfo":{"status":"ok","timestamp":1602682797259,"user_tz":-420,"elapsed":32381,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"f629a693-cf0c-4143-e82b-234b44bac8f7","colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["# then creating a dictionary\n","label_dict = {}\n","for index, possible_label in enumerate(possible_labels):\n","    label_dict[possible_label] = index\n","label_dict"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'angry': 2,\n"," 'disgust': 3,\n"," 'happy': 0,\n"," 'not-relevant': 1,\n"," 'sad': 4,\n"," 'surprise': 5}"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"QL17741oSdkV","executionInfo":{"status":"ok","timestamp":1602682797260,"user_tz":-420,"elapsed":32373,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"d6f60f6d-8e28-4fd3-95b3-1f321dc51378","colab":{"base_uri":"https://localhost:8080/","height":280}},"source":["# now adding another column of the label that has just been created to the dataframe\n","df['label'] = df.category.replace(label_dict)\n","df.head()\n"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>category</th>\n","      <th>label</th>\n","    </tr>\n","    <tr>\n","      <th>id</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>614484565059596288</th>\n","      <td>Dorian Gray with Rainbow Scarf #LoveWins (from...</td>\n","      <td>happy</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>614746522043973632</th>\n","      <td>@SelectShowcase @Tate_StIves ... Replace with ...</td>\n","      <td>happy</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>614877582664835073</th>\n","      <td>@Sofabsports thank you for following me back. ...</td>\n","      <td>happy</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>611932373039644672</th>\n","      <td>@britishmuseum @TudorHistory What a beautiful ...</td>\n","      <td>happy</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>611570404268883969</th>\n","      <td>@NationalGallery @ThePoldarkian I have always ...</td>\n","      <td>happy</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                                 text  ... label\n","id                                                                     ...      \n","614484565059596288  Dorian Gray with Rainbow Scarf #LoveWins (from...  ...     0\n","614746522043973632  @SelectShowcase @Tate_StIves ... Replace with ...  ...     0\n","614877582664835073  @Sofabsports thank you for following me back. ...  ...     0\n","611932373039644672  @britishmuseum @TudorHistory What a beautiful ...  ...     0\n","611570404268883969  @NationalGallery @ThePoldarkian I have always ...  ...     0\n","\n","[5 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"9QV4uMNETdFd"},"source":["## Task 3: Training/Validation Split"]},{"cell_type":"code","metadata":{"id":"7DajpH50ThAC","executionInfo":{"status":"ok","timestamp":1602682798026,"user_tz":-420,"elapsed":33131,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}}},"source":["from sklearn.model_selection import train_test_split"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"C9y_iP4RTpq2","executionInfo":{"status":"ok","timestamp":1602682798027,"user_tz":-420,"elapsed":33127,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}}},"source":["X_train, X_val, y_train, y_val = train_test_split(\n","    # index value - the unique identification for each sample\n","    df.index.values,\n","\n","    # label here for randomly splitting the labels\n","    df.label.values,\n","\n","    # choosing 15% of the test size\n","    test_size = 0.15,\n","\n","    # choosing random state\n","    random_state=17,\n","\n","    # because there are more than 2 labels and there is a hugh discrepancy\n","    # between the number of each of the labels\n","    # therefore, choosing stratifying random sampling\n","    # for the splitting training/test sets\n","    stratify=df.label.values\n",")"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"18j-9pFwXfgo","executionInfo":{"status":"ok","timestamp":1602682798027,"user_tz":-420,"elapsed":33122,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"f9e4df7a-367b-4fad-9788-aae5f09cb077","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# number of samples\n","df.shape[0]"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1481"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"mFZcQ1eVTuf2","executionInfo":{"status":"ok","timestamp":1602682798028,"user_tz":-420,"elapsed":33115,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"3f49a8ae-2cae-40f0-dcd3-95d97aadd954","colab":{"base_uri":"https://localhost:8080/","height":315}},"source":["# creating a new column 'data_type'\n","df['data_type'] = ['not_set']*df.shape[0]\n","df.head()"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>category</th>\n","      <th>label</th>\n","      <th>data_type</th>\n","    </tr>\n","    <tr>\n","      <th>id</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>614484565059596288</th>\n","      <td>Dorian Gray with Rainbow Scarf #LoveWins (from...</td>\n","      <td>happy</td>\n","      <td>0</td>\n","      <td>not_set</td>\n","    </tr>\n","    <tr>\n","      <th>614746522043973632</th>\n","      <td>@SelectShowcase @Tate_StIves ... Replace with ...</td>\n","      <td>happy</td>\n","      <td>0</td>\n","      <td>not_set</td>\n","    </tr>\n","    <tr>\n","      <th>614877582664835073</th>\n","      <td>@Sofabsports thank you for following me back. ...</td>\n","      <td>happy</td>\n","      <td>0</td>\n","      <td>not_set</td>\n","    </tr>\n","    <tr>\n","      <th>611932373039644672</th>\n","      <td>@britishmuseum @TudorHistory What a beautiful ...</td>\n","      <td>happy</td>\n","      <td>0</td>\n","      <td>not_set</td>\n","    </tr>\n","    <tr>\n","      <th>611570404268883969</th>\n","      <td>@NationalGallery @ThePoldarkian I have always ...</td>\n","      <td>happy</td>\n","      <td>0</td>\n","      <td>not_set</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                                 text  ... data_type\n","id                                                                     ...          \n","614484565059596288  Dorian Gray with Rainbow Scarf #LoveWins (from...  ...   not_set\n","614746522043973632  @SelectShowcase @Tate_StIves ... Replace with ...  ...   not_set\n","614877582664835073  @Sofabsports thank you for following me back. ...  ...   not_set\n","611932373039644672  @britishmuseum @TudorHistory What a beautiful ...  ...   not_set\n","611570404268883969  @NationalGallery @ThePoldarkian I have always ...  ...   not_set\n","\n","[5 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"3g4xZjpZT4Hz","executionInfo":{"status":"ok","timestamp":1602682798028,"user_tz":-420,"elapsed":33107,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}}},"source":["# now setting values 'train' and 'val' for the column 'data_type\n","df.loc[X_train, 'data_type'] = 'train'\n","df.loc[X_val, 'data_type'] = 'val'"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"P7e1f7dIUAdJ","executionInfo":{"status":"ok","timestamp":1602682798029,"user_tz":-420,"elapsed":33103,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"ab311989-3ee7-406b-f53f-0b5dad40553d","colab":{"base_uri":"https://localhost:8080/","height":438}},"source":["# now seeing the results of splitting by the groupby function\n","df.groupby(['category', 'label', 'data_type']).count()"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th>text</th>\n","    </tr>\n","    <tr>\n","      <th>category</th>\n","      <th>label</th>\n","      <th>data_type</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th rowspan=\"2\" valign=\"top\">angry</th>\n","      <th rowspan=\"2\" valign=\"top\">2</th>\n","      <th>train</th>\n","      <td>48</td>\n","    </tr>\n","    <tr>\n","      <th>val</th>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"2\" valign=\"top\">disgust</th>\n","      <th rowspan=\"2\" valign=\"top\">3</th>\n","      <th>train</th>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>val</th>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"2\" valign=\"top\">happy</th>\n","      <th rowspan=\"2\" valign=\"top\">0</th>\n","      <th>train</th>\n","      <td>966</td>\n","    </tr>\n","    <tr>\n","      <th>val</th>\n","      <td>171</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"2\" valign=\"top\">not-relevant</th>\n","      <th rowspan=\"2\" valign=\"top\">1</th>\n","      <th>train</th>\n","      <td>182</td>\n","    </tr>\n","    <tr>\n","      <th>val</th>\n","      <td>32</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"2\" valign=\"top\">sad</th>\n","      <th rowspan=\"2\" valign=\"top\">4</th>\n","      <th>train</th>\n","      <td>27</td>\n","    </tr>\n","    <tr>\n","      <th>val</th>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"2\" valign=\"top\">surprise</th>\n","      <th rowspan=\"2\" valign=\"top\">5</th>\n","      <th>train</th>\n","      <td>30</td>\n","    </tr>\n","    <tr>\n","      <th>val</th>\n","      <td>5</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                              text\n","category     label data_type      \n","angry        2     train        48\n","                   val           9\n","disgust      3     train         5\n","                   val           1\n","happy        0     train       966\n","                   val         171\n","not-relevant 1     train       182\n","                   val          32\n","sad          4     train        27\n","                   val           5\n","surprise     5     train        30\n","                   val           5"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"TdtJ51w38A1D","executionInfo":{"status":"ok","timestamp":1602682798029,"user_tz":-420,"elapsed":33095,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"d0f42e70-e1c5-4c03-a293-abe7077e2538","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# number of samples of training data\n","len(df[df['data_type'] == 'train'])"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1258"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"gXk72opS8IVn","executionInfo":{"status":"ok","timestamp":1602682798030,"user_tz":-420,"elapsed":33087,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"a6d75ab8-2551-4ea6-b5c0-86b9712b93e0","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# number of samples of test data\n","len(df[df['data_type'] == 'val'])"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["223"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"t3hQ6k6lYxC7"},"source":["## Task 4: Loading Tokenizer and Encoding our Data"]},{"cell_type":"code","metadata":{"id":"BHpeRzBVZA3-","executionInfo":{"status":"ok","timestamp":1602682803917,"user_tz":-420,"elapsed":38966,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"37766d8f-b1a1-4bd8-fed9-2ad0c0a3e335","colab":{"base_uri":"https://localhost:8080/","height":625}},"source":["!pip install transformers"],"execution_count":22,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/22/aff234f4a841f8999e68a7a94bdd4b60b4cebcfeca5d67d61cd08c9179de/transformers-3.3.1-py3-none-any.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 2.8MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Collecting tokenizers==0.8.1.rc2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n","\u001b[K     |████████████████████████████████| 3.0MB 18.3MB/s \n","\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Collecting sentencepiece!=0.1.92\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 31.6MB/s \n","\u001b[?25hCollecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 44.9MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=2aed5f4deacbd151892dc0337214139ad06be5e8198f5cbc68b335672c8f1209\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc2 transformers-3.3.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"elqXkq3iY8-L","executionInfo":{"status":"ok","timestamp":1602682805861,"user_tz":-420,"elapsed":40901,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}}},"source":["from transformers import BertTokenizer\n","from torch.utils.data import TensorDataset # this is for setting data usable for pytorch tensor later"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"e1ncJizFZMER","executionInfo":{"status":"ok","timestamp":1602682808136,"user_tz":-420,"elapsed":43171,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"3da435b3-09d1-4ef3-8d02-8c29197f3c99","colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["0b3349765b17428bb79608dadb1386d5","75346e6d42d94e6e850a4e8ad503588f","88256d9a9f964ad492cf040393e9ce20","90275722015c45adbc16ffa87f8e0990","7ea30b5fc2f343c4ac3a0c894c8852e0","3a26853122854d35b24c3fcbaf3ecdd3","28368ac3b43b4afaac767910be8084b6","70ee573481dc420c803c5a67e52b2a10"]}},"source":["tokenizer = BertTokenizer.from_pretrained(\n","    # choosing uncased version of BERT\n","    # all of the characters being lower case\n","    'bert-base-uncased', \n","\n","    # thus, converting everything to lower case\n","    do_lower_case=True)"],"execution_count":24,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0b3349765b17428bb79608dadb1386d5","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tphZ7qSyZiAj","executionInfo":{"status":"ok","timestamp":1602682808137,"user_tz":-420,"elapsed":43164,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"77b4d948-7765-4289-84da-143111e04f25","colab":{"base_uri":"https://localhost:8080/","height":89}},"source":["# FOR TRAINING DATA\n","encoded_data_train = tokenizer.batch_encode_plus(\n","\n","    # the actual sentences from the training data as the input\n","    df[df.data_type == 'train'].text.values,\n","    \n","    # for BERT to know where the sentence ends and new one begins \n","    add_special_tokens=True,\n","    \n","    # because you use the fixed input, we need everything having the same dimensionality\n","    # in this case, we choose max length 256 in the following argument\n","    # therefore, when putting attention mask for different sentences having different lengths \n","    # the lengths are now having the same leghth will produce 1 or 0 saying where the actual words are\n","    # 1 will be where the words are, 0 will be not the words \n","    return_attention_mask=True,\n","    \n","    # pad all sentences to a certain max length\n","    # then we have to set the following argument max_length a certain value\n","    pad_to_max_length=True,\n","    \n","    # setting the max length of 256 because the tweet is unlikely to be longer than 256 characters\n","    max_length=256,\n","    \n","    # returning pytorch tensors as the outcome\n","    return_tensors='pt' \n","    )"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1773: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"DekxKCVQaeJ7","executionInfo":{"status":"ok","timestamp":1602682808138,"user_tz":-420,"elapsed":43156,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"9158b739-e805-4bc2-aa6a-51102542803a","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# encoded_data_train behaves like a dict although the printing result is something else\n","type(encoded_data_train)"],"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["transformers.tokenization_utils_base.BatchEncoding"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"Vgu53kAmaxSS","executionInfo":{"status":"ok","timestamp":1602682808138,"user_tz":-420,"elapsed":43148,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"45e9f349-ffa5-4138-866e-9e7461687db4","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# we can retrieve the keys of encoded_data_train\n","encoded_data_train.keys()"],"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"tqsjc0n63NsM","executionInfo":{"status":"ok","timestamp":1602682808897,"user_tz":-420,"elapsed":43898,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"d13681c7-66e2-4adb-f1d9-d7ea7da9ad06","colab":{"base_uri":"https://localhost:8080/","height":155}},"source":["# examining the input_ids of encoded_data_train\n","print(encoded_data_train['input_ids'].shape)\n","encoded_data_train['input_ids']"],"execution_count":28,"outputs":[{"output_type":"stream","text":["torch.Size([1258, 256])\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["tensor([[  101, 16092,  3897,  ...,     0,     0,     0],\n","        [  101,  1030, 27034,  ...,     0,     0,     0],\n","        [  101,  1030, 10682,  ...,     0,     0,     0],\n","        ...,\n","        [  101, 11047,  1030,  ...,     0,     0,     0],\n","        [  101,  1030,  3680,  ...,     0,     0,     0],\n","        [  101,  1030,  2120,  ...,     0,     0,     0]])"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"0FV7FogS3Xn6","executionInfo":{"status":"ok","timestamp":1602682808900,"user_tz":-420,"elapsed":43892,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"9ba9855e-416d-44f6-ffba-1349aafa0701","colab":{"base_uri":"https://localhost:8080/","height":155}},"source":["# examining the token_type_ids of encoded_data_train\n","print(encoded_data_train['token_type_ids'].shape)\n","encoded_data_train['token_type_ids']"],"execution_count":29,"outputs":[{"output_type":"stream","text":["torch.Size([1258, 256])\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["tensor([[0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        ...,\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0]])"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"jRlNfV7I7s_i","executionInfo":{"status":"ok","timestamp":1602682808901,"user_tz":-420,"elapsed":43884,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"0eba9266-0a7f-471c-9db5-7d74d4a1f2ae","colab":{"base_uri":"https://localhost:8080/","height":155}},"source":["# examining the attention_mask of encoded_data_train\n","print(encoded_data_train['attention_mask'].shape)\n","encoded_data_train['attention_mask']"],"execution_count":30,"outputs":[{"output_type":"stream","text":["torch.Size([1258, 256])\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]])"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"SDni0akh_jqk","executionInfo":{"status":"ok","timestamp":1602682808901,"user_tz":-420,"elapsed":43875,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}}},"source":["# input_ids_train -> each word as a number\n","input_ids_train = encoded_data_train['input_ids']\n","\n","# the position 1 & 0 where there are words and where there are not\n","attention_masks_train = encoded_data_train['attention_mask']\n","\n","# this is just derived from the training dataframe\n","labels_train = torch.tensor(df[df.data_type=='train'].label.values)"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"id":"p_7-T6IV_82B","executionInfo":{"status":"ok","timestamp":1602682808902,"user_tz":-420,"elapsed":43870,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"e81a8926-fa93-449a-9d99-21975e09859b","colab":{"base_uri":"https://localhost:8080/","height":504}},"source":["print(input_ids_train.shape)\n","input_ids_train[0]"],"execution_count":32,"outputs":[{"output_type":"stream","text":["torch.Size([1258, 256])\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["tensor([  101, 16092,  3897,  2007, 10098, 18982,  1001,  2293, 10105,  2015,\n","         1006,  2013,  1030,  2329,  7606, 14820,  8299,  1024,  1013,  1013,\n","         1056,  1012,  2522,  1013,  1053,  2549,  2595, 26760,  2140,  2692,\n","         2229,  2226,  1007,  8299,  1024,  1013,  1013,  1056,  1012,  2522,\n","         1013,  1044,  2692,  6777, 19279,  2497, 13088,  4160,   102,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0])"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"glywSj4oD5pn","executionInfo":{"status":"ok","timestamp":1602682808902,"user_tz":-420,"elapsed":43862,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"69bef201-fc60-4a1f-f156-9a0898dfa075","colab":{"base_uri":"https://localhost:8080/","height":245}},"source":["print(attention_masks_train.shape)\n","attention_masks_train[0]"],"execution_count":33,"outputs":[{"output_type":"stream","text":["torch.Size([1258, 256])\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"1aFfO-MfFN_V","executionInfo":{"status":"ok","timestamp":1602682808903,"user_tz":-420,"elapsed":43854,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"2364809a-1f60-42f7-e211-96490e5e2c0f","colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["print(labels_train.shape)\n","labels_train"],"execution_count":34,"outputs":[{"output_type":"stream","text":["torch.Size([1258])\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["tensor([0, 0, 0,  ..., 0, 0, 1])"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"code","metadata":{"id":"-sEc7uzM-rwG","executionInfo":{"status":"ok","timestamp":1602682808903,"user_tz":-420,"elapsed":43846,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"18c9de1f-b8cb-434d-e34e-f5a4ebe8cb32","colab":{"base_uri":"https://localhost:8080/","height":89}},"source":["# FOR VALIDATION DATA\n","encoded_data_val = tokenizer.batch_encode_plus(\n","\n","    # the actual sentences from the training data as the input\n","    df[df.data_type == 'val'].text.values,\n","    \n","    # for BERT to know where the sentence ends and new one begins \n","    add_special_tokens=True,\n","    \n","    # because you use the fixed input, we need everything having the same dimensionality\n","    # in this case, we choose max length 256 in the following argument\n","    # therefore, when putting attention mask for different sentences having different lengths \n","    # the lengths are now having the same leghth will produce 1 or 0 saying where the actual words are\n","    # 1 will be where the words are, 0 will be not the words \n","    return_attention_mask=True,\n","    \n","    # pad all sentences to a certain max length\n","    # then we have to set the following argument max_length a certain value\n","    pad_to_max_length=True,\n","    \n","    # setting the max length of 256 because the tweet is unlikely to be longer than 256 characters\n","    max_length=256,\n","    \n","    # returning pytorch tensors as the outcome\n","    return_tensors='pt' \n","    )"],"execution_count":35,"outputs":[{"output_type":"stream","text":["Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1773: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"YldLg6xuFgC8","executionInfo":{"status":"ok","timestamp":1602682808905,"user_tz":-420,"elapsed":43840,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}}},"source":["# input_ids_val -> each word as a number\n","input_ids_val = encoded_data_val['input_ids']\n","\n","# the position 1 & 0 where there are words and where there are not\n","attention_masks_val = encoded_data_val['attention_mask']\n","\n","# this is just derived from the training dataframe\n","labels_val = torch.tensor(df[df.data_type=='val'].label.values)"],"execution_count":36,"outputs":[]},{"cell_type":"code","metadata":{"id":"3yaOYUfFGAKC","executionInfo":{"status":"ok","timestamp":1602682808906,"user_tz":-420,"elapsed":43834,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}}},"source":["# NOW CREATING TWO DATASETS IN TENSOR FORMS\n","dataset_train = TensorDataset(input_ids_train,\n","                              attention_masks_train,\n","                              labels_train)\n","\n","dataset_val = TensorDataset(input_ids_val,\n","                            attention_masks_val,\n","                            labels_val)"],"execution_count":37,"outputs":[]},{"cell_type":"code","metadata":{"id":"5kNUFPNeJKQb","executionInfo":{"status":"ok","timestamp":1602682808906,"user_tz":-420,"elapsed":43829,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"90690b02-2e89-4f5a-f514-b7a9219ad2d8","colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["# getting lengths for those two datasets\n","print(len(dataset_train))\n","print(len(dataset_val))"],"execution_count":38,"outputs":[{"output_type":"stream","text":["1258\n","223\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jHrO77auGY0l","executionInfo":{"status":"ok","timestamp":1602682808907,"user_tz":-420,"elapsed":43819,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"e6da6032-0da7-471f-f117-1e0e8a36b8e7","colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["# getting the type of the datasets of training and validation\n","print(type(dataset_train))\n","print(type(dataset_val))"],"execution_count":39,"outputs":[{"output_type":"stream","text":["<class 'torch.utils.data.dataset.TensorDataset'>\n","<class 'torch.utils.data.dataset.TensorDataset'>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"U05dg1r9JJ3j","executionInfo":{"status":"ok","timestamp":1602682808907,"user_tz":-420,"elapsed":43812,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"648ebb5c-8b5b-4357-fa0d-873888aa3115","colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["# elements in the training or validation dataset has tupple type\n","print(type(dataset_train[0]))\n","print(type(dataset_val[0]))"],"execution_count":40,"outputs":[{"output_type":"stream","text":["<class 'tuple'>\n","<class 'tuple'>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"C3Du071iIskf","executionInfo":{"status":"ok","timestamp":1602682808908,"user_tz":-420,"elapsed":43805,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"39ad7ed9-e3f8-425c-b816-b64f7f0cfede","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# examining number of components in one element\n","# they include input_ids_train, attention_mask_train & labels_train\n","len(dataset_train[0])"],"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"code","metadata":{"id":"VMi-JENTLEK9","executionInfo":{"status":"ok","timestamp":1602682808908,"user_tz":-420,"elapsed":43797,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"02ca8d55-fbad-47d1-bbe2-0d8d79eea45d","colab":{"base_uri":"https://localhost:8080/","height":694}},"source":["# one element in the dataset_train has 3 components including \n","# input_ids, attention_mask & label in that order \n","dataset_train[0]"],"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([  101, 16092,  3897,  2007, 10098, 18982,  1001,  2293, 10105,  2015,\n","          1006,  2013,  1030,  2329,  7606, 14820,  8299,  1024,  1013,  1013,\n","          1056,  1012,  2522,  1013,  1053,  2549,  2595, 26760,  2140,  2692,\n","          2229,  2226,  1007,  8299,  1024,  1013,  1013,  1056,  1012,  2522,\n","          1013,  1044,  2692,  6777, 19279,  2497, 13088,  4160,   102,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0]),\n"," tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n"," tensor(0))"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"markdown","metadata":{"id":"BbwjIuAwLPk7"},"source":["## Task 5: Setting up BERT Pretrained Model"]},{"cell_type":"code","metadata":{"id":"LdmRW6lvLQ52","executionInfo":{"status":"ok","timestamp":1602682808909,"user_tz":-420,"elapsed":43791,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}}},"source":["# importing pretrained BERT model\n","# we're treating each tweet as its own unique sequence \n","# so one sequence will be classified into one of six classes\n","from transformers import BertForSequenceClassification"],"execution_count":43,"outputs":[]},{"cell_type":"code","metadata":{"id":"bprXYvzbMV5I","executionInfo":{"status":"ok","timestamp":1602682819176,"user_tz":-420,"elapsed":54053,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"f59a9e79-f027-49c8-8622-e8a31eb81bd4","colab":{"base_uri":"https://localhost:8080/","height":222,"referenced_widgets":["de574041486a4047bffb2ad6d7b11066","9358b68c78bc4835b66d902ef46ed521","67b9a1a344564a16aee1222b04e6903b","09b061fc707b45408ca3818910e4a548","0df254e16bd94a4294658d250d0c588e","c484e8d4995e47f2aa77c923e2eeea5e","4f1f33a9135b4938948fc6d8acc21ed5","5679ed8ad84f47cfb169058eb8406714","82821b2396e04fb7b0bd7d07a27aa265","726562a64a3c49d9bc91c672bd4c0338","b809eb851dda487995bb586f0dc78548","3d6c5333179845ec94722e13e6b6f038","fd305f6954bc40a692be6c7db90abef9","21337a6168744fecb1dbea442798067d","1d027baa1ea7484980811b0446892fbe","2b3371751d4e4276b781e78b4b726a73"]}},"source":["# This step is to redefine the architecture to include parts that we need\n","model = BertForSequenceClassification.from_pretrained(\n","    # reason for using this is computationally efficient \n","    # it is a small version \n","    'bert-base-uncased',\n","\n","    # how many output labels the final layout of BERT must have \n","    # will be able to classify \n","    num_labels = len(label_dict),\n","\n","    # we dont actually want any unnecessary outputs from the model\n","    output_attentions = False,\n","\n","    # we don't care the output of the hidden state which is \n","    # the state just before the prediction \n","    # this might be useful for encoding situations, but for us, we don't really need them\n","    output_hidden_states = False\n",")"],"execution_count":44,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"de574041486a4047bffb2ad6d7b11066","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"82821b2396e04fb7b0bd7d07a27aa265","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"LUjCYGdoEUtI","executionInfo":{"status":"ok","timestamp":1602682819177,"user_tz":-420,"elapsed":54046,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"1f6131ee-0bbd-4876-b8a2-a6ca515caff8","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# now investigating the model\n","# by printing out the model, we can see that the model is huge\n","# from the huggingface info, https://huggingface.co/transformers/pretrained_models.html \n","# the model is 12-layer, 768-hidden, 12-heads, 110M parameters. Trained on lower-cased English text.\n","print(model)"],"execution_count":45,"outputs":[{"output_type":"stream","text":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OaC4ee5CGCYf","executionInfo":{"status":"ok","timestamp":1602682819177,"user_tz":-420,"elapsed":54038,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"6da75386-ae67-41e5-9aac-5f2b0629ee3b","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# type of the model\n","type(model)"],"execution_count":46,"outputs":[{"output_type":"execute_result","data":{"text/plain":["transformers.modeling_bert.BertForSequenceClassification"]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"code","metadata":{"id":"1Fyf5sUcGH0h","executionInfo":{"status":"ok","timestamp":1602682819178,"user_tz":-420,"elapsed":54032,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"9435eefe-6841-43f5-a7f1-71395b3d9b91","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# now look at model parameters\n","model.parameters()"],"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<generator object Module.parameters at 0x7fce910b2570>"]},"metadata":{"tags":[]},"execution_count":47}]},{"cell_type":"markdown","metadata":{"id":"NVmLjdabPlHR"},"source":["## Task 6: Creating Data Loader"]},{"cell_type":"code","metadata":{"id":"FHAIngRxPpIR","executionInfo":{"status":"ok","timestamp":1602682819178,"user_tz":-420,"elapsed":54024,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}}},"source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler"],"execution_count":48,"outputs":[]},{"cell_type":"code","metadata":{"id":"SdjJHAYbQAW6","executionInfo":{"status":"ok","timestamp":1602682819179,"user_tz":-420,"elapsed":54020,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}}},"source":["# normally we can use the batch size 32\n","# however, in this case, we use 4 because of \n","# the limited resources including memory and hardware\n","batch_size = 32  #32\n","\n","dataloader_train = DataLoader(\n","    dataset_train,\n","    # this is preventing model to learn sequentially differences when training\n","    sampler=RandomSampler(dataset_train),\n","    batch_size=batch_size\n",")\n","\n","dataloader_val = DataLoader(\n","    dataset_val,\n","    sampler=RandomSampler(dataset_val),\n","    # setting back the batch size to 32 \n","    # since we don't need to do any back propagation\n","    batch_size=32\n",")"],"execution_count":49,"outputs":[]},{"cell_type":"code","metadata":{"id":"7IEq1qNm63mC","executionInfo":{"status":"ok","timestamp":1602682819179,"user_tz":-420,"elapsed":54015,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"fca016dc-fcdb-4b7b-d58e-add95137f5f1","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["type(dataloader_train)"],"execution_count":50,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.utils.data.dataloader.DataLoader"]},"metadata":{"tags":[]},"execution_count":50}]},{"cell_type":"code","metadata":{"id":"a3t42OI4672E","executionInfo":{"status":"ok","timestamp":1602682819180,"user_tz":-420,"elapsed":54008,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"a503ad18-d81a-48f0-b43c-536e2477458c","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# the number of dataloader_train, 315, is smaller than the number of dataset_train, 1258\n","# this could be because the sampling process which gets smaller size for the dataloader_train\n","len(dataloader_train)"],"execution_count":51,"outputs":[{"output_type":"execute_result","data":{"text/plain":["40"]},"metadata":{"tags":[]},"execution_count":51}]},{"cell_type":"code","metadata":{"id":"FGbDkXeL8rVf","executionInfo":{"status":"ok","timestamp":1602682819181,"user_tz":-420,"elapsed":54000,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"e94fea3c-0a94-41a0-f2b3-6164341eae7d","colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["# there is no direct way to indexing the dataloader\n","# thus using for loop to get the items\n","# in this case, the item's type is a list\n","n = 0\n","for i in dataloader_train:\n","    if n <= 2:\n","        print(type(i))\n","        n+=1"],"execution_count":52,"outputs":[{"output_type":"stream","text":["<class 'list'>\n","<class 'list'>\n","<class 'list'>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WzoL_V45_y3M","executionInfo":{"status":"ok","timestamp":1602682819181,"user_tz":-420,"elapsed":53992,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"14bf5a12-e242-46ef-a45f-05b7f7c0068b","colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["# now we investigate number of elements in each list\n","# there are 3 elements in each list\n","n = 0\n","for i in dataloader_train:\n","    if n <= 2:\n","        print(len(i))\n","        n+=1"],"execution_count":53,"outputs":[{"output_type":"stream","text":["3\n","3\n","3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_ddeVO2GATK7","executionInfo":{"status":"ok","timestamp":1602682819182,"user_tz":-420,"elapsed":53984,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"4de8a018-c1af-46f0-9eda-6267a598947f","colab":{"base_uri":"https://localhost:8080/","height":279}},"source":["# now we print out only one item in dataloader by setting n <= 0\n","# the items has a form of 3 tensors including input_ids, attention_mask & labels\n","n = 0\n","for i in dataloader_train:\n","    if n <= 0:\n","        print(i)\n","        n+=1"],"execution_count":54,"outputs":[{"output_type":"stream","text":["[tensor([[ 101, 1996, 3819,  ...,    0,    0,    0],\n","        [ 101, 1030, 3680,  ...,    0,    0,    0],\n","        [ 101, 1045, 2074,  ...,    0,    0,    0],\n","        ...,\n","        [ 101, 2256, 1001,  ...,    0,    0,    0],\n","        [ 101, 1030, 9902,  ...,    0,    0,    0],\n","        [ 101, 1012, 1030,  ...,    0,    0,    0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), tensor([0, 0, 2, 0, 1, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,\n","        0, 0, 0, 0, 0, 0, 1, 0])]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2wbf5oFVBvUR","executionInfo":{"status":"ok","timestamp":1602682819183,"user_tz":-420,"elapsed":53977,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"c0ca20c5-b15b-4c71-cdc9-87ef14b18c67","colab":{"base_uri":"https://localhost:8080/","height":314}},"source":["# now we print out only one item in dataloader by setting n <= 0\n","# the items has a form of 3 tensors including input_ids, attention_mask & labels\n","n = 0\n","for i in dataloader_train:\n","    if n <= 0:\n","        print(i[0])\n","        print(i[1])\n","        print(i[2])\n","        n+=1"],"execution_count":55,"outputs":[{"output_type":"stream","text":["tensor([[  101,  1030, 14753,  ...,     0,     0,     0],\n","        [  101,  1030,  2329,  ...,     0,     0,     0],\n","        [  101,  1045,  2018,  ...,     0,     0,     0],\n","        ...,\n","        [  101,  1030,  2120,  ...,     0,     0,     0],\n","        [  101,  1037,  2210,  ...,     0,     0,     0],\n","        [  101,  6854,  1996,  ...,     0,     0,     0]])\n","tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]])\n","tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 2, 0, 0,\n","        0, 1, 0, 1, 0, 1, 0, 4])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9T6dzlsaB6L8","executionInfo":{"status":"ok","timestamp":1602682819183,"user_tz":-420,"elapsed":53969,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"bf7aa3f6-6e28-4c4b-93cf-c16a95fbeb0e","colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["# now investigate the dimension of each of the elements in one item\n","# we can see that the first element, input_ids, has a dimension of 4x256 where 4 is the number of batch_size and 256 is the number of the chosen max_length\n","# similarly, the attention_mask, the second element also has a dimension of 4x256\n","# finally, the labels have a dimension of 4 as the number of batch_size\n","n = 0\n","for i in dataloader_train:\n","    if n <= 0:\n","        print(i[0].shape)\n","        print(i[1].shape)\n","        print(i[2].shape)\n","        n+=1"],"execution_count":56,"outputs":[{"output_type":"stream","text":["torch.Size([32, 256])\n","torch.Size([32, 256])\n","torch.Size([32])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"C4CpQ8QYRU88"},"source":["## Task 7: Setting Up Optimizer and Scheduler"]},{"cell_type":"code","metadata":{"id":"tlFtXsC3RaQa","executionInfo":{"status":"ok","timestamp":1602682819184,"user_tz":-420,"elapsed":53962,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}}},"source":["from transformers import AdamW, get_linear_schedule_with_warmup"],"execution_count":57,"outputs":[]},{"cell_type":"code","metadata":{"id":"HuGdAcnTRlAY","executionInfo":{"status":"ok","timestamp":1602682819184,"user_tz":-420,"elapsed":53957,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}}},"source":["# now we do optimisation, more info from this post https://huggingface.co/transformers/main_classes/optimizer_schedules.html\n","# in this case, we choose AdamW which is \n","optimizer = AdamW(\n","    # model's parameters - Iterable of parameters to optimize or dictionaries defining parameter groups.\n","    model.parameters(),\n","\n","    # learning rate\n","    lr=1e-5, #2e-5 > 5e-5\n","\n","    # Is a very small number to prevent any division by zero in the implementation (e.g. 10E-8).\n","    # source: https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/\n","    eps=1e-8\n",")"],"execution_count":58,"outputs":[]},{"cell_type":"code","metadata":{"id":"3ef-JSFUM7IE","executionInfo":{"status":"ok","timestamp":1602682819185,"user_tz":-420,"elapsed":53953,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"d3be539b-50d9-457e-8d9c-5b96a21f9792","colab":{"base_uri":"https://localhost:8080/","height":173}},"source":["print(type(optimizer))\n","optimizer"],"execution_count":59,"outputs":[{"output_type":"stream","text":["<class 'transformers.optimization.AdamW'>\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["AdamW (\n","Parameter Group 0\n","    betas: (0.9, 0.999)\n","    correct_bias: True\n","    eps: 1e-08\n","    lr: 1e-05\n","    weight_decay: 0.0\n",")"]},"metadata":{"tags":[]},"execution_count":59}]},{"cell_type":"code","metadata":{"id":"bhrMf5vySkCA","executionInfo":{"status":"ok","timestamp":1602682819185,"user_tz":-420,"elapsed":53942,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}}},"source":["epochs = 10\n","\n","# Create a schedule with a learning rate that decreases linearly from \n","# the initial lr set in the optimizer to 0, after a warmup period during \n","# which it increases linearly from 0 to the initial lr set in the optimizer.\n","\n","# Warm up steps is a parameter which is used to lower the learning rate \n","# in order to reduce the impact of deviating the model \n","# from learning on sudden new data set exposure.\n","# source: https://stackoverflow.com/questions/60120043/optimizer-and-scheduler-for-bert-fine-tuning\n","scheduler = get_linear_schedule_with_warmup(\n","    # The optimizer for which to schedule the learning rate.\n","    optimizer,\n","\n","    # The number of steps for the warmup phase\n","    num_warmup_steps=0,\n","\n","    # The total number of training steps.\n","    num_training_steps=len(dataloader_train)*epochs\n",")"],"execution_count":60,"outputs":[]},{"cell_type":"code","metadata":{"id":"QHG00tbgL21e","executionInfo":{"status":"ok","timestamp":1602682819185,"user_tz":-420,"elapsed":53936,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"529b8e3b-bb32-4c02-d0df-7a37bf15cae4","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["len(dataloader_train)*epochs"],"execution_count":61,"outputs":[{"output_type":"execute_result","data":{"text/plain":["400"]},"metadata":{"tags":[]},"execution_count":61}]},{"cell_type":"markdown","metadata":{"id":"oL7lXui4S31U"},"source":["## Task 8: Defining our Performance Metrics"]},{"cell_type":"markdown","metadata":{"id":"g22JTEpo-4wq"},"source":["Accuracy metric approach originally used in accuracy function in this [tutorial](https://mccormickml.com/2019/07/22/BERT-fine-tuning/)\n"]},{"cell_type":"code","metadata":{"id":"_WAe9f17_ELD","executionInfo":{"status":"ok","timestamp":1602682819186,"user_tz":-420,"elapsed":53929,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}}},"source":["import numpy as np"],"execution_count":62,"outputs":[]},{"cell_type":"code","metadata":{"id":"r4iFBntY_IQG","executionInfo":{"status":"ok","timestamp":1602682819186,"user_tz":-420,"elapsed":53924,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}}},"source":["from sklearn.metrics import f1_score"],"execution_count":63,"outputs":[]},{"cell_type":"code","metadata":{"id":"bxZjeGICKIty","executionInfo":{"status":"ok","timestamp":1602682819187,"user_tz":-420,"elapsed":53920,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}}},"source":["# predictions will come in a form of a list-like of 6 values representing probabilities distribution\n","# preds = [0.9, 0.05, 0.05, 0, 0, 0]\n","\n","# but we want it to become\n","# preds = [1, 0, 0, 0, 0, 0]"],"execution_count":64,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Ief6Yrx_Iuf","executionInfo":{"status":"ok","timestamp":1602682819187,"user_tz":-420,"elapsed":53915,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}}},"source":["def f1_score_func(preds, labels):\n","    # we want the flat value of preditions, we don't want the list of lists\n","    # thus we use flatten()\n","    preds_flat = np.argmax(preds, axis=1).flatten()\n","\n","    # we do the same things with labels \n","    labels_flat = labels.flatten()\n","\n","    # then we use f1_score from sklearn to calculate with weighted average\n","    return f1_score(labels_flat, preds_flat, average='weighted')"],"execution_count":65,"outputs":[]},{"cell_type":"code","metadata":{"id":"29eOWIXd_I73","executionInfo":{"status":"ok","timestamp":1602682819188,"user_tz":-420,"elapsed":53911,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}}},"source":["# for example, taking all labels of class 5 and let see how many class 5 being predicted\n","def accuracy_per_class(preds, labels):\n","    label_dict_inverse = {v: k for k,v in label_dict.items()}\n","\n","    preds_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","\n","    for label in np.unique(labels_flat):\n","        y_preds = preds_flat[labels_flat==label]\n","        y_true = labels_flat[labels_flat==label]\n","        print(f'Class: {label_dict_inverse[label]}')\n","        print(f'Accuracy: {len(y_preds[y_preds==label])/len(y_true)}\\n')\n"],"execution_count":66,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"idiETRRcQMO9"},"source":["## Task 9: Creating our Training Loop"]},{"cell_type":"code","metadata":{"id":"FXzng0mRQU-k","executionInfo":{"status":"ok","timestamp":1602682819188,"user_tz":-420,"elapsed":53906,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}}},"source":["import random\n","\n","seed_val = 17\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)"],"execution_count":67,"outputs":[]},{"cell_type":"code","metadata":{"id":"rJF0XvXVRcah","executionInfo":{"status":"ok","timestamp":1602682833320,"user_tz":-420,"elapsed":68033,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"6d1c2f5a-afac-47ec-87a5-5170cef69621","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","print(device)"],"execution_count":68,"outputs":[{"output_type":"stream","text":["cuda\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IHTYxMRcSzzp","executionInfo":{"status":"ok","timestamp":1602682833321,"user_tz":-420,"elapsed":68025,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}}},"source":["def evaluate(dataloader_val):\n","\n","    model.eval()\n","    \n","    loss_val_total = 0\n","    predictions, true_vals = [], []    \n","    \n","    for batch in dataloader_val:\n","        \n","        batch = tuple(b.to(device) for b in batch)\n","        \n","        inputs = {'input_ids':      batch[0],\n","                  'attention_mask': batch[1],\n","                  'labels':         batch[2],\n","                 }\n","\n","        with torch.no_grad():        \n","            outputs = model(**inputs)\n","            \n","        loss = outputs[0]\n","        logits = outputs[1]\n","        loss_val_total += loss.item()\n","        \n","        logits = logits.detach().cpu().numpy()\n","        label_ids = inputs['labels'].cpu().numpy()\n","        predictions.append(logits)\n","        true_vals.append(label_ids)\n","    \n","    loss_val_avg = loss_val_total/len(dataloader_val) \n","    \n","    predictions = np.concatenate(predictions, axis=0)\n","    true_vals = np.concatenate(true_vals, axis=0)\n","            \n","    return loss_val_avg, predictions, true_vals"],"execution_count":69,"outputs":[]},{"cell_type":"code","metadata":{"id":"RkyiuKKJeT0F","executionInfo":{"status":"ok","timestamp":1602683193986,"user_tz":-420,"elapsed":428685,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"f906f571-fb2e-427a-8dab-7fcf21411038","colab":{"base_uri":"https://localhost:8080/","height":930,"referenced_widgets":["1afdbf82790e43c2837438f80875c78a","e2851b9779004738b9fad09a4eb837ce","33b4c70c6931469d882be4280f135184","72129be4fe044434adbfff68bcf8ebff","9fa4ea8eb0af40b2bd53172bdde0c47b","0a25fcf07bc14bfdb693b3eaa6a0a62c","6fc02aa420404b9ea4f5cc220023f014","8cd5b14d63c04168b42ba0ab856f8f79","bf0b865efaa04b658bb4328b3f480af2","8ec1e48ac84e4aa79c8b24023de6853d","7133d9e9934f4af9a870fa15966e6ce7","84faaf1372cb4934bb98e856477af8a7","617c8ad2c851437c9cda21736ede59f8","0e81cfd1116b4be08de0152a9c7981e0","e69fbb5667e340f3bb5de4e581269e5c","067fb02baeed4072aa6acb085431b42c","1e2c6fa236ad48429c3922cf6fb3147c","7239a55baa1c465f9e7af21c45910310","045f532970f84314be5d2b0bfc2f8286","26b5a48600214bd4aa51410cc483df44","9637e0dc8bde4a7ca1fc86c075ab2c1b","850b5017aa1940da80c215c7d90b082f","83aee442e18c4c74938d267788e8a9dc","71773ba203d042799eea8a183fa0f17a","17d3ad976b094819832113c22445e33c","4c22846f86e74c0ca1f834eef5a67358","57024425657c46a7a5f72038da199960","e3d6ff1a080344aea6a8a9b3e7f0382a","fe675e807b5d458281fdb40717ebf794","d92412b129fc4aebb0af011b5fe4185e","4753ee97ecc54f57acb3dc6415d1ebf9","e3e0fa7bfd374ff0836d06b9643b5c8c","eb579ce308014c9d876ebf2fab20dec0","f502d1b58dd9443393c3b361f915ed5e","0dad2f4711104690990bea2a8633adf8","67cf3149c4044dc0a4966ce3b1cf2549","7b97d45cd3104fd181fd274249fc7862","687194a2d19a48638e4424f9fe6d0f7b","b4c807c2e4e24683a0333236c1a6ec13","f2a61c7075b246848ee79b58dbbabe4b","a2d5fffdb8904ec48eb2f7f49420e4ce","c3634aa125f7416fa4bb6f3b302dc4fd","d2a2b5e583d541e4ae7d9b198054e83a","040ede06e9ad404ea811f2c5c676918c","dad8bfe53d3e4d0a91b9bc8077bd53d5","858800004a31480eb4a2f029fe39a5ef","68d9f656d5524afc9eff8ee451f85f6b","36962c374b7e4d1e9717a3b6d4cb34d5","ac4c5b6bec0743899cc7d452435acbf4","0143e5a95da248c281952f377cb4302a","4b343d916a7448e1b5b57bdbaeff51cb","70c6110a3d8449ad8f5fd6674f30da79","974f786bc2ec4021a36c37622a813ee0","97313c3713a04981b261dee02172b67f","9991f80c940f46af9c7ac96e21ab26f8","299bbc871f6e4c7c9040b364c20b98f4","bb04bf8df8504a0fa47a1cfe7f94cd87","7d05fad76f8c406c9fe134272a8529f6","49b0c690eeb94a6f848c7b57d8d04e68","2a0d6955171c4b849b0b89f921b24fa5","48a6ec78a2aa4897b7be5d42b5b85d42","e2d8f84dda204afaa4d83a6ef9677fe8","c89f2f878f284a699deeddc2d7869c5d","916884d6d5e74840b97003c61e1c05b3","793b914f535245a991866133ff4f3a9a","d731f44a83534a119f5e12eaa300ec48","3595f76b537c4ff997aa27919bd6419f","ae3cfa0361624b09b25700d2bb151368","a9e27bb879be4226b12a830ee4d83263","6c5623a6e1b844d9a0c0b65b82e1ffdd","6e6e2568fed94e3dbfc72f37826b7a06","09d5e605016343688ada38b90de21f9d","ac85eedc899748f1bf3e181dc583a026","9271177d79904a158ca4f1cb043bb7db","16b6b8d2a7084918bbe8dc9dff118ea8","3a5d821234294111a2d7f0b7864f87a8","60f328b20ea34b7684556d30069b6c4f","7f80a09d760e4d5b9d5662b45da9e1fc","89d04c7fcfda400aab44787b0bfbb94e","1be5843f40d94132bf31b646376b4ab5","1ccdb58e45b74058b443aa89b1a0e920","efa2b28238304c6f92ec14d89d3e71a8","b6fa04f82751418fb8654be0632490dd","0f81abf07e074618bc8f1bf9eab964e8","7753e8a45678470a958de2e47afe937b","10bd83627b2c4990a61f475c7f85a661","3467a1db3019490da992a3e332dc218c","e957da41330d4ab5acf7f0e3deeba804"]}},"source":["# the training loops\n","for epoch in tqdm(range(1, epochs + 1)):\n","    \n","    # stating model in the training mode\n","    model.train()\n","\n","    # setting training loss to zero\n","    # each epoch getting average training loss then we add each batch's loss onto this variable\n","    loss_train_total = 0\n","\n","    # using progress bar method from tqdm to see how many batches have been trained and how many to go\n","    progress_bar = tqdm(dataloader_train, \n","                        desc='Epoch {:1d}'.format(epoch),\n","                        # leave = False --> overwrite itself after each epoch\n","                        leave = False, \n","                        disable=False)\n","    \n","    # refering to the progress_bar above, this is the wrapping method \n","    # which means progress_bar is now the dataloader_train \n","    # it is illustrated in the function demo below\n","    for batch in progress_bar:\n","        \n","        # the standard pytorch procedure, need to set zero gradient for model\n","        model.zero_grad()\n","\n","        # reminder: elements of dataloader_train having 3 different variables in the tuple form\n","        # now we try to ensure each of individual items of a single tuple on the correct device whether cuda or cpu\n","        batch = tuple(b.to(device) for b in batch)\n","\n","        # now we pull out the inputs in the format of dictionary, our inputs is what to go to the model\n","        inputs = {\n","            'input_ids':       batch[0],\n","            'attention_mask':  batch[1],\n","            'labels':          batch[2]\n","        }\n","        \n","        # now we get the output coming from the input and the model\n","        outputs = model(**inputs)\n","\n","        # the first output of the BERT training model is the loss\n","        # thus we assign the model loss to the variable loss \n","        loss = outputs[0]\n","\n","        # then we add up the loss to loss_train_total\n","        loss_train_total += loss.item()\n","\n","        # then we do back propagation on loss\n","        loss.backward()\n","\n","        # clipping gradients into a norm value, in this case it is 1.0 \n","        # to prevent the gradients vanishing or exploding\n","        torch.nn.utils.clip_grad_value_(model.parameters(), 1.0)\n","\n","        # to update optimizer\n","        optimizer.step()\n","\n","        # to update scheduler\n","        scheduler.step()\n","\n","        # update progress_bar to show the loss per batch\n","        progress_bar.set_postfix({'training loss':  '{:.3f}'.format(loss.item()/len(batch))})\n","\n","    # save the model per epoch\n","    torch.save(model.state_dict(), directory_path + f'/models/BERT_ft_{epoch}.model')\n","\n","    # report few more things\n","    tqdm.write(f'\\nEpoch {epoch}')\n","\n","    loss_train_avg = loss_train_total/len(dataloader_train)\n","    tqdm.write(f'Training loss:  {loss_train_avg}')\n","\n","    val_loss, predictions, true_vals = evaluate(dataloader_val)\n","    val_f1 = f1_score_func(predictions, true_vals)\n","    tqdm.write(f'Validation loss: {val_loss}')\n","    tqdm.write(f'F1 score (weighted): {val_f1}')"],"execution_count":70,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1afdbf82790e43c2837438f80875c78a","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bf0b865efaa04b658bb4328b3f480af2","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Epoch 1', max=40.0, style=ProgressStyle(description_width…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\r\n","Epoch 1\n","Training loss:  1.0725305318832397\n","Validation loss: 0.7962499175752912\n","F1 score (weighted): 0.7136675190820192\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1e2c6fa236ad48429c3922cf6fb3147c","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Epoch 2', max=40.0, style=ProgressStyle(description_width…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\r\n","Epoch 2\n","Training loss:  0.7259105630218983\n","Validation loss: 0.6924787291458675\n","F1 score (weighted): 0.7385602518843621\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"17d3ad976b094819832113c22445e33c","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Epoch 3', max=40.0, style=ProgressStyle(description_width…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\r\n","Epoch 3\n","Training loss:  0.5748166345059872\n","Validation loss: 0.6455750209944588\n","F1 score (weighted): 0.7684224498198171\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"eb579ce308014c9d876ebf2fab20dec0","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Epoch 4', max=40.0, style=ProgressStyle(description_width…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\r\n","Epoch 4\n","Training loss:  0.4956930406391621\n","Validation loss: 0.5946802794933319\n","F1 score (weighted): 0.7790288504974604\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a2d5fffdb8904ec48eb2f7f49420e4ce","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Epoch 5', max=40.0, style=ProgressStyle(description_width…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\r\n","Epoch 5\n","Training loss:  0.4328852526843548\n","Validation loss: 0.5864090940782002\n","F1 score (weighted): 0.7816587566858171\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ac4c5b6bec0743899cc7d452435acbf4","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Epoch 6', max=40.0, style=ProgressStyle(description_width…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\r\n","Epoch 6\n","Training loss:  0.3896180383861065\n","Validation loss: 0.5758848403181348\n","F1 score (weighted): 0.780976441609297\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bb04bf8df8504a0fa47a1cfe7f94cd87","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Epoch 7', max=40.0, style=ProgressStyle(description_width…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\r\n","Epoch 7\n","Training loss:  0.3513915497809649\n","Validation loss: 0.5923969873360225\n","F1 score (weighted): 0.7862480072500582\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"793b914f535245a991866133ff4f3a9a","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Epoch 8', max=40.0, style=ProgressStyle(description_width…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\r\n","Epoch 8\n","Training loss:  0.35097981058061123\n","Validation loss: 0.5779805545295987\n","F1 score (weighted): 0.7858849957352656\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ac85eedc899748f1bf3e181dc583a026","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Epoch 9', max=40.0, style=ProgressStyle(description_width…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\r\n","Epoch 9\n","Training loss:  0.3247888036072254\n","Validation loss: 0.5813159389155251\n","F1 score (weighted): 0.7954993058692611\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1ccdb58e45b74058b443aa89b1a0e920","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Epoch 10', max=40.0, style=ProgressStyle(description_widt…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\r\n","Epoch 10\n","Training loss:  0.3161234274506569\n","Validation loss: 0.5795356120382037\n","F1 score (weighted): 0.7954993058692611\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"45_nP3_zXT_Y"},"source":["## Task 10: Loading and Evaluating our Model"]},{"cell_type":"code","metadata":{"id":"PSTxru8rXTIz","executionInfo":{"status":"ok","timestamp":1602683521247,"user_tz":-420,"elapsed":5611,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"f5d164d1-7af5-46a4-baee-26a671c0eeb9","colab":{"base_uri":"https://localhost:8080/","height":124}},"source":["model = BertForSequenceClassification.from_pretrained(\n","    'bert-base-uncased',\n","    # how many output labels the final layout of BERT must have \n","    # will be able to classify \n","    num_labels = len(label_dict),\n","\n","    # we dont actually want any unnecessary outputs from the model\n","    output_attentions = False,\n","\n","    # we don't care the output of the hidden state which is \n","    # the state just before the prediction \n","    # this might be useful for encoding situations, but for us, we don't really need them\n","    output_hidden_states = False\n",")"],"execution_count":74,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"5K52LRgeYdIg","executionInfo":{"status":"ok","timestamp":1602683829752,"user_tz":-420,"elapsed":1195,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"bfb0bff5-2cc2-49d2-ef29-e1b0dd179efc","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["model.to(device)"],"execution_count":76,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":76}]},{"cell_type":"code","metadata":{"id":"aXWkqYAmZpjY","executionInfo":{"status":"ok","timestamp":1602683924210,"user_tz":-420,"elapsed":2811,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"b1f1b46c-53d6-4c0f-a708-40c941de20cf","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["model.load_state_dict(\n","    torch.load(\n","        directory_path + '/models/BERT_ft_10.model',\n","        map_location=torch.device(device)\n","    )\n",")"],"execution_count":77,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":77}]},{"cell_type":"code","metadata":{"id":"E1ynumZYaBJg","executionInfo":{"status":"ok","timestamp":1602683981632,"user_tz":-420,"elapsed":2941,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}}},"source":["loss_val_avg, predictions, true_vals = evaluate(dataloader_val)"],"execution_count":78,"outputs":[]},{"cell_type":"code","metadata":{"id":"UWitx8guaOXN","executionInfo":{"status":"ok","timestamp":1602684006478,"user_tz":-420,"elapsed":1236,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"c6ed8003-1f26-4341-e092-7d40be6edf28","colab":{"base_uri":"https://localhost:8080/","height":328}},"source":["accuracy_per_class(predictions,true_vals)"],"execution_count":79,"outputs":[{"output_type":"stream","text":["Class: happy\n","Accuracy: 0.9532163742690059\n","\n","Class: not-relevant\n","Accuracy: 0.6875\n","\n","Class: angry\n","Accuracy: 0.0\n","\n","Class: disgust\n","Accuracy: 0.0\n","\n","Class: sad\n","Accuracy: 0.0\n","\n","Class: surprise\n","Accuracy: 0.0\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0flmdkYNjloc"},"source":["\n","\n","**function demo for the chuck of code above**"]},{"cell_type":"code","metadata":{"id":"6dRFgHXvfUbw","executionInfo":{"status":"ok","timestamp":1602683193987,"user_tz":-420,"elapsed":428684,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"21f96026-fe97-4300-bdd9-aa419d10e2de","colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["c6f250489d954d48808cddab6b325813","2b27d40c56de4dbfaabb8582f1a660be","5299bdcc17eb4b51953ecd2672d24989","446eb9d25cd64664b51f74f0929e9d03","39d985768c0f4787a748d2260024cf20","ff02ba3724974a2d8c5a6a2ec8d62b0c","657dc1de8dfc47d2bc706d84429e001b","ee39d31abfd8425faf426262bb188ebb"]}},"source":["# tqdm function is to show the progess bar as each training epoch going through\n","tqdm(range(1,epochs+1))"],"execution_count":71,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c6f250489d954d48808cddab6b325813","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"execute_result","data":{"text/plain":["  0%|<bar/>| 0/10 [00:00<?, ?it/s]"]},"metadata":{"tags":[]},"execution_count":71}]},{"cell_type":"code","metadata":{"id":"MvmqzRgchlTm","executionInfo":{"status":"ok","timestamp":1602683193988,"user_tz":-420,"elapsed":428683,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"84104e03-51e4-4cc9-fab7-e3d711b70875","colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["cfeddb91faf54274a7daee7a48636faf","7a47cd32adb74061b4ef69d3a860850d","5039179b211541cab48a49d287563b08","dcfcc89092d84a86a3249ad7d8e75a0d","df647f5947614092bc17630553bf541d","abf4e078c29b49b09930f9ee5754adba","76b9a1234f1440bfb4831b8058813c2f","b594307020f54b76a2f94b9945f5d14c"]}},"source":["epoch = 3\n","tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch))"],"execution_count":72,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cfeddb91faf54274a7daee7a48636faf","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Epoch 3', max=40.0, style=ProgressStyle(description_width…"]},"metadata":{"tags":[]}},{"output_type":"execute_result","data":{"text/plain":["Epoch 3:   0%|<bar/>| 0/40 [00:00<?, ?it/s]"]},"metadata":{"tags":[]},"execution_count":72}]},{"cell_type":"code","metadata":{"id":"9-1Tm30uikLW","executionInfo":{"status":"ok","timestamp":1602683193989,"user_tz":-420,"elapsed":428682,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}},"outputId":"24277e8d-b885-4ac9-87b3-ba929f6a86ca","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["73899fe9a41543f18afbe83fdbc7fe1f","c424479433184da18c6ec6449f1316bc","f8e1d281baff460b99826b0205ea1de1","e7ca93142ea04337b0e4f701f82c3e7c","ab14104299fd44bfbdd102b6e65dd205","472eb0903b484357b0034d7c4b10dff3","3046241f362a444aa68b4f0063676e39","51f6afe2c1b34d8999ce9da264a385fc"]}},"source":["# from this, we can see that the prog_bar is now the dataloader_train\n","prog_bar = tqdm(dataloader_train)\n","for batch1 in prog_bar:\n","    print(batch1)"],"execution_count":73,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"73899fe9a41543f18afbe83fdbc7fe1f","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=40.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["[tensor([[  101,  4957,  3972,  ...,     0,     0,     0],\n","        [  101,  1015,  2733,  ...,     0,     0,     0],\n","        [  101,  1030,  8223,  ...,     0,     0,     0],\n","        ...,\n","        [  101,  2559,  2830,  ...,     0,     0,     0],\n","        [  101,  1012,  1030,  ...,     0,     0,     0],\n","        [  101,  1030, 21113,  ...,     0,     0,     0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), tensor([1, 1, 0, 0, 1, 2, 0, 0, 1, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n","        0, 0, 2, 0, 0, 0, 0, 4])]\n","[tensor([[  101,  4760,  2115,  ...,     0,     0,     0],\n","        [  101,  1030, 17623,  ...,     0,     0,     0],\n","        [  101,  6284,  2660,  ...,     0,     0,     0],\n","        ...,\n","        [  101,  1030, 11937,  ...,     0,     0,     0],\n","        [  101,  4292,  2039,  ...,     0,     0,     0],\n","        [  101,  6854,  1996,  ...,     0,     0,     0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), tensor([0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n","        0, 0, 1, 0, 1, 0, 0, 4])]\n","[tensor([[  101,  1998,  1037,  ...,     0,     0,     0],\n","        [  101,  1001,  6284,  ...,     0,     0,     0],\n","        [  101,  2307,  2305,  ...,     0,     0,     0],\n","        ...,\n","        [  101,  3752,  1030,  ...,     0,     0,     0],\n","        [  101,  1001, 21877,  ...,     0,     0,     0],\n","        [  101,  2076,  2115,  ...,     0,     0,     0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 5, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1,\n","        0, 1, 0, 0, 0, 0, 0, 0])]\n","[tensor([[  101,  1030,  2120,  ...,     0,     0,     0],\n","        [  101,  1030,  2585,  ...,     0,     0,     0],\n","        [  101,  2307,  2831,  ...,     0,     0,     0],\n","        ...,\n","        [  101,  1030,  2329,  ...,     0,     0,     0],\n","        [  101,  2028,  1997,  ...,     0,     0,     0],\n","        [  101, 14657,  2000,  ...,     0,     0,     0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), tensor([0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 4,\n","        5, 0, 0, 0, 0, 0, 0, 2])]\n","[tensor([[  101,  1030,  1035,  ...,     0,     0,     0],\n","        [  101,  1030,  8194,  ...,     0,     0,     0],\n","        [  101,  1030, 11503,  ...,     0,     0,     0],\n","        ...,\n","        [  101,  1000, 23751,  ...,     0,     0,     0],\n","        [  101,  2293, 15544,  ...,     0,     0,     0],\n","        [  101,  7568,  2000,  ...,     0,     0,     0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), tensor([0, 1, 5, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 1, 0, 0, 0])]\n","[tensor([[ 101, 1030, 4205,  ...,    0,    0,    0],\n","        [ 101, 7823, 2335,  ...,    0,    0,    0],\n","        [ 101, 1030, 2688,  ...,    0,    0,    0],\n","        ...,\n","        [ 101, 1030, 2329,  ...,    0,    0,    0],\n","        [ 101, 2253, 2000,  ...,    0,    0,    0],\n","        [ 101, 1030, 2381,  ...,    0,    0,    0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), tensor([0, 0, 1, 0, 0, 4, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 2, 4, 0, 0, 5, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0])]\n","[tensor([[  101,  5294, 24240,  ...,     0,     0,     0],\n","        [  101, 10166,  2027,  ...,     0,     0,     0],\n","        [  101,  1030,  8223,  ...,     0,     0,     0],\n","        ...,\n","        [  101,  1030,  6728,  ...,     0,     0,     0],\n","        [  101,  2307,  6609,  ...,     0,     0,     0],\n","        [  101,  5621,  8403,  ...,     0,     0,     0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), tensor([4, 0, 1, 1, 0, 0, 0, 0, 0, 4, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n","        0, 0, 0, 3, 0, 2, 0, 0])]\n","[tensor([[  101,  1030, 14753,  ...,     0,     0,     0],\n","        [  101,  1030,  2329,  ...,     0,     0,     0],\n","        [  101,  2307,  2203,  ...,     0,     0,     0],\n","        ...,\n","        [  101,  1030,  2329,  ...,     0,     0,     0],\n","        [  101,  2200, 16625,  ...,     0,     0,     0],\n","        [  101,  1001,  2293,  ...,     0,     0,     0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), tensor([0, 2, 0, 0, 0, 0, 0, 0, 0, 4, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0])]\n","[tensor([[ 101, 2128, 1011,  ...,    0,    0,    0],\n","        [ 101, 2551, 3116,  ...,    0,    0,    0],\n","        [ 101, 1001, 3198,  ...,    0,    0,    0],\n","        ...,\n","        [ 101, 1030, 2120,  ...,    0,    0,    0],\n","        [ 101, 1030, 6583,  ...,    0,    0,    0],\n","        [ 101, 2057, 1521,  ...,    0,    0,    0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n","        0, 0, 0, 0, 0, 1, 0, 0])]\n","[tensor([[  101,  1030, 11260,  ...,     0,     0,     0],\n","        [  101,  2095,  1019,  ...,     0,     0,     0],\n","        [  101,  1030,  2120,  ...,     0,     0,     0],\n","        ...,\n","        [  101,  1030,  9902,  ...,     0,     0,     0],\n","        [  101,  2256,  3319,  ...,     0,     0,     0],\n","        [  101,  4121,  2264,  ...,     0,     0,     0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), tensor([0, 0, 0, 0, 1, 4, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 5,\n","        0, 0, 1, 0, 0, 0, 0, 0])]\n","[tensor([[  101, 26049,  2012,  ...,     0,     0,     0],\n","        [  101,  5632,  1996,  ...,     0,     0,     0],\n","        [  101,  2329,  7606,  ...,     0,     0,     0],\n","        ...,\n","        [  101,  1030,  2455,  ...,     0,     0,     0],\n","        [  101,  2026, 12944,  ...,     0,     0,     0],\n","        [  101,  1030,  3680,  ...,     0,     0,     0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), tensor([2, 0, 0, 0, 1, 1, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n","        0, 0, 0, 1, 0, 1, 0, 0])]\n","[tensor([[  101,  1996,  5622,  ...,     0,     0,     0],\n","        [  101,  2220,  2851,  ...,     0,     0,     0],\n","        [  101,  2057,  2024,  ...,     0,     0,     0],\n","        ...,\n","        [  101,  1030, 22421,  ...,     0,     0,     0],\n","        [  101,  1030,  3347,  ...,     0,     0,     0],\n","        [  101,  1012,  1030,  ...,     0,     0,     0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), tensor([0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 1, 5, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0])]\n","[tensor([[  101,  1037,  3835,  ...,     0,     0,     0],\n","        [  101,  2023,  5048,  ...,     0,     0,     0],\n","        [  101,  5939,  1030,  ...,     0,     0,     0],\n","        ...,\n","        [  101, 16092,  3897,  ...,     0,     0,     0],\n","        [  101,  1996,  1001,  ...,     0,     0,     0],\n","        [  101,  2182,  1005,  ...,     0,     0,     0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 5, 0,\n","        0, 0, 4, 0, 0, 0, 0, 0])]\n","[tensor([[  101,  2559,  2830,  ...,     0,     0,     0],\n","        [  101,  1045,  1005,  ...,     0,     0,     0],\n","        [  101,  1030,  2047,  ...,     0,     0,     0],\n","        ...,\n","        [  101,  1030,  5968,  ...,     0,     0,     0],\n","        [  101, 18168,  2290,  ...,     0,     0,     0],\n","        [  101,  1030,  4348,  ...,     0,     0,     0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n","        1, 0, 0, 0, 0, 0, 5, 1])]\n","[tensor([[  101,  2559,  2830,  ...,     0,     0,     0],\n","        [  101,  8403, 10537,  ...,     0,     0,     0],\n","        [  101,  1030,  2120,  ...,     0,     0,     0],\n","        ...,\n","        [  101,  3407,  2000,  ...,     0,     0,     0],\n","        [  101,  1030,  2120,  ...,     0,     0,     0],\n","        [  101,  1030,  7087,  ...,     0,     0,     0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), tensor([0, 0, 2, 2, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 1,\n","        0, 0, 0, 0, 2, 0, 1, 1])]\n","[tensor([[  101,  2651,  1024,  ...,     0,     0,     0],\n","        [  101,  2018,  1996,  ...,     0,     0,     0],\n","        [  101,  1030,  2329,  ...,     0,     0,     0],\n","        ...,\n","        [  101,  6397,  7516,  ...,     0,     0,     0],\n","        [  101, 17797,  6209,  ...,     0,     0,     0],\n","        [  101,  1030, 22421,  ...,     0,     0,     0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), tensor([0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n","        0, 1, 0, 0, 0, 0, 0, 0])]\n","[tensor([[ 101, 2227, 1996,  ...,    0,    0,    0],\n","        [ 101, 1030, 2120,  ...,    0,    0,    0],\n","        [ 101, 1030, 2329,  ...,    0,    0,    0],\n","        ...,\n","        [ 101, 1996, 2064,  ...,    0,    0,    0],\n","        [ 101, 1030, 1039,  ...,    0,    0,    0],\n","        [ 101, 1030, 2981,  ...,    0,    0,    0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), tensor([0, 1, 0, 0, 0, 0, 1, 1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n","        0, 5, 0, 0, 0, 0, 0, 0])]\n","[tensor([[  101,  1030, 10625,  ...,     0,     0,     0],\n","        [  101,  1030, 16776,  ...,     0,     0,     0],\n","        [  101,  1030, 11374,  ...,     0,     0,     0],\n","        ...,\n","        [  101,  2057,  2024,  ...,     0,     0,     0],\n","        [  101, 14726,   999,  ...,     0,     0,     0],\n","        [  101,  1030,  6583,  ...,     0,     0,     0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 5, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 2, 0, 2,\n","        0, 1, 0, 0, 0, 1, 0, 0])]\n","[tensor([[  101,  1030,  8223,  ...,     0,     0,     0],\n","        [  101,  2559,  2830,  ...,     0,     0,     0],\n","        [  101,  1037,  8348,  ...,     0,     0,     0],\n","        ...,\n","        [  101,  2028,  1997,  ...,     0,     0,     0],\n","        [  101,  1030, 28425,  ...,     0,     0,     0],\n","        [  101,  4283,  2000,  ...,     0,     0,     0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), tensor([1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 4, 0, 0, 1, 1, 0, 0, 1, 0, 5, 0, 0,\n","        0, 0, 0, 0, 0, 0, 3, 0])]\n","[tensor([[  101,  2057,  2020,  ...,     0,     0,     0],\n","        [  101, 14657,  1030,  ...,     0,     0,     0],\n","        [  101,  3945,  2005,  ...,     0,     0,     0],\n","        ...,\n","        [  101,  2089,  2031,  ...,     0,     0,     0],\n","        [  101,  2314,  3340,  ...,     0,     0,     0],\n","        [  101,  2184,  1001,  ...,     0,     0,     0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), tensor([0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0,\n","        0, 0, 0, 0, 0, 0, 0, 1])]\n","[tensor([[ 101, 1001, 6284,  ...,    0,    0,    0],\n","        [ 101, 1030, 6647,  ...,    0,    0,    0],\n","        [ 101, 1030, 2120,  ...,    0,    0,    0],\n","        ...,\n","        [ 101, 2210, 2978,  ...,    0,    0,    0],\n","        [ 101, 4067, 2017,  ...,    0,    0,    0],\n","        [ 101, 1030, 2329,  ...,    0,    0,    0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), tensor([0, 1, 2, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 4, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 5])]\n","[tensor([[  101,  2026,  4268,  ...,     0,     0,     0],\n","        [  101,  1996, 28071,  ...,     0,     0,     0],\n","        [  101,  9902,  1024,  ...,     0,     0,     0],\n","        ...,\n","        [  101, 10474,  1053,  ...,     0,     0,     0],\n","        [  101,  1030, 18178,  ...,     0,     0,     0],\n","        [  101,  1030,  2120,  ...,     0,     0,     0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), tensor([0, 1, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n","        1, 0, 0, 0, 0, 1, 0, 0])]\n","[tensor([[  101,  2175,  2065,  ...,     0,     0,     0],\n","        [  101,  1030,  7059,  ...,     0,     0,     0],\n","        [  101,  1030,  9348,  ...,     0,     0,     0],\n","        ...,\n","        [  101,  1001, 21461,  ...,     0,     0,     0],\n","        [  101,  1030,  2120,  ...,     0,     0,     0],\n","        [  101,  1001,  6335,  ...,     0,     0,     0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), tensor([0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 1, 0, 1, 0, 0, 0, 5, 0, 1, 0, 0, 1,\n","        0, 0, 0, 0, 0, 1, 5, 0])]\n","[tensor([[  101,  3811, 16755,  ...,     0,     0,     0],\n","        [  101,  1030,  7920,  ...,     0,     0,     0],\n","        [  101,  1020,  2086,  ...,     0,     0,     0],\n","        ...,\n","        [  101,  1030,  2120,  ...,     0,     0,     0],\n","        [  101,  2508,  9712,  ...,     0,     0,     0],\n","        [  101,  1012,  1030,  ...,     0,     0,     0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n","        0, 0, 1, 0, 0, 0, 0, 3])]\n","[tensor([[  101,  1045,  9075,  ...,     0,     0,     0],\n","        [  101,  2182,  1005,  ...,     0,     0,     0],\n","        [  101,  1030,  2120,  ...,     0,     0,     0],\n","        ...,\n","        [  101,  1030, 20996,  ...,     0,     0,     0],\n","        [  101,  1030,  5639,  ...,     0,     0,     0],\n","        [  101,  2138,  2651,  ...,     0,     0,     0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), tensor([2, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 4, 0, 0,\n","        1, 0, 1, 0, 0, 0, 0, 0])]\n","[tensor([[  101,  1030, 10682,  ...,     0,     0,     0],\n","        [  101, 12327, 13463,  ...,     0,     0,     0],\n","        [  101,  1037,  2200,  ...,     0,     0,     0],\n","        ...,\n","        [  101,  1030,  2329,  ...,     0,     0,     0],\n","        [  101,  1030, 16420,  ...,     0,     0,     0],\n","        [  101,  2559,  2830,  ...,     0,     0,     0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,\n","        0, 0, 1, 0, 0, 0, 0, 0])]\n","[tensor([[  101,  2005,  2396,  ...,     0,     0,     0],\n","        [  101,  1030,  7354,  ...,     0,     0,     0],\n","        [  101,  1030, 11503,  ...,     0,     0,     0],\n","        ...,\n","        [  101,  2074,  1037,  ...,     0,     0,     0],\n","        [  101,  1030,  2120,  ...,     0,     0,     0],\n","        [  101,  2307,  4495,  ...,     0,     0,     0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), tensor([0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 1, 0, 4, 4, 0, 1, 0, 0, 1, 2, 0, 0, 4,\n","        0, 0, 0, 0, 1, 0, 0, 0])]\n","[tensor([[ 101, 2644, 1996,  ...,    0,    0,    0],\n","        [ 101, 1030, 2120,  ...,    0,    0,    0],\n","        [ 101, 1001, 3198,  ...,    0,    0,    0],\n","        ...,\n","        [ 101, 2028, 1997,  ...,    0,    0,    0],\n","        [ 101, 1030, 2329,  ...,    0,    0,    0],\n","        [ 101, 6919, 1030,  ...,    0,    0,    0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), tensor([2, 1, 2, 0, 0, 0, 0, 0, 0, 0, 5, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n","        1, 0, 0, 0, 0, 0, 0, 0])]\n","[tensor([[  101,  1001,  3198,  ...,     0,     0,     0],\n","        [  101,  1045, 12246,  ...,     0,     0,     0],\n","        [  101,  1030, 21871,  ...,     0,     0,     0],\n","        ...,\n","        [  101,  2018,  2107,  ...,     0,     0,     0],\n","        [  101,  1030,  1996,  ...,     0,     0,     0],\n","        [  101,  7632,  1030,  ...,     0,     0,     0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), tensor([2, 0, 3, 0, 0, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n","        0, 0, 0, 2, 0, 0, 3, 1])]\n","[tensor([[  101,  1030,  3203,  ...,     0,     0,     0],\n","        [  101,  1030, 18373,  ...,     0,     0,     0],\n","        [  101,  1030,  3680,  ...,     0,     0,     0],\n","        ...,\n","        [  101,  7568,  2055,  ...,     0,     0,     0],\n","        [  101,  1030,  9902,  ...,     0,     0,     0],\n","        [  101,  2893,  7568,  ...,     0,     0,     0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), tensor([1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 4, 0, 1, 1, 1, 0,\n","        0, 5, 0, 0, 2, 0, 0, 0])]\n","[tensor([[ 101, 6581, 2695,  ...,    0,    0,    0],\n","        [ 101, 1030, 1038,  ...,    0,    0,    0],\n","        [ 101, 1030, 3841,  ...,    0,    0,    0],\n","        ...,\n","        [ 101, 1030, 2329,  ...,    0,    0,    0],\n","        [ 101, 2131, 2115,  ...,    0,    0,    0],\n","        [ 101, 1030, 2643,  ...,    0,    0,    0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), tensor([0, 0, 0, 0, 0, 1, 0, 5, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 5, 0, 0, 1, 1, 0])]\n","[tensor([[  101,  2307,  2000,  ...,     0,     0,     0],\n","        [  101,  2651,  1005,  ...,     0,     0,     0],\n","        [  101,  1030,  2120,  ...,     0,     0,     0],\n","        ...,\n","        [  101,  2329,  7606,  ...,     0,     0,     0],\n","        [  101,  2307,  2000,  ...,     0,     0,     0],\n","        [  101,  2851, 21392,  ...,     0,     0,     0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), tensor([0, 0, 4, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n","        0, 0, 0, 2, 0, 0, 0, 0])]\n","[tensor([[ 101, 1030, 2329,  ...,    0,    0,    0],\n","        [ 101, 1030, 2120,  ...,    0,    0,    0],\n","        [ 101, 7078, 3866,  ...,    0,    0,    0],\n","        ...,\n","        [ 101, 1030, 2329,  ...,    0,    0,    0],\n","        [ 101, 4299, 1030,  ...,    0,    0,    0],\n","        [ 101, 1030, 4550,  ...,    0,    0,    0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), tensor([5, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n","        0, 0, 0, 0, 5, 5, 4, 0])]\n","[tensor([[  101,  2009,  1005,  ...,     0,     0,     0],\n","        [  101,  1012,  1030,  ...,     0,     0,     0],\n","        [  101,  1030,  2308,  ...,     0,     0,     0],\n","        ...,\n","        [  101,  1030,  2198,  ...,     0,     0,     0],\n","        [  101,  1001, 12854,  ...,     0,     0,     0],\n","        [  101,  1030,  4035,  ...,     0,     0,     0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 5, 0,\n","        0, 0, 0, 0, 1, 0, 0, 4])]\n","[tensor([[  101,  5667,  4086,  ...,     0,     0,     0],\n","        [  101,  1999,  2414,  ...,     0,     0,     0],\n","        [  101,  1030, 22421,  ...,     0,     0,     0],\n","        ...,\n","        [  101,  2725,  2470,  ...,     0,     0,     0],\n","        [  101,  1030,  4459,  ...,     0,     0,     0],\n","        [  101,  1030, 17306,  ...,     0,     0,     0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 2, 1, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0])]\n","[tensor([[  101,  1037,  2986,  ...,     0,     0,     0],\n","        [  101,  1030,  2120,  ...,     0,     0,     0],\n","        [  101,  4283,  2005,  ...,     0,     0,     0],\n","        ...,\n","        [  101,  1030,  2120,  ...,     0,     0,     0],\n","        [  101, 12854,  2256,  ...,     0,     0,     0],\n","        [  101,  1030,  2120,  ...,     0,     0,     0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), tensor([0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 1, 0, 0, 0, 5, 0, 1])]\n","[tensor([[ 101, 2125, 2000,  ...,    0,    0,    0],\n","        [ 101, 1030, 1041,  ...,    0,    0,    0],\n","        [ 101, 1030, 2120,  ...,    0,    0,    0],\n","        ...,\n","        [ 101, 9119, 1030,  ...,    0,    0,    0],\n","        [ 101, 1030, 2329,  ...,    0,    0,    0],\n","        [ 101, 1996, 2739,  ...,    0,    0,    0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 2, 0, 0, 0, 0, 1, 0,\n","        0, 0, 0, 1, 0, 4, 0, 0])]\n","[tensor([[  101,  1030,  2120,  ...,     0,     0,     0],\n","        [  101,  1030,  2329,  ...,     0,     0,     0],\n","        [  101,  1030,  2329,  ...,     0,     0,     0],\n","        ...,\n","        [  101,  1030,  2585,  ...,     0,     0,     0],\n","        [  101,  9107,  1996,  ...,     0,     0,     0],\n","        [  101,  1030, 28425,  ...,     0,     0,     0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n","        0, 0, 0, 1, 0, 0, 0, 1])]\n","[tensor([[  101,  1030,  1047,  ...,     0,     0,     0],\n","        [  101,  1030,  2329,  ...,     0,     0,     0],\n","        [  101,  1030,  1035,  ...,     0,     0,     0],\n","        ...,\n","        [  101, 17834, 14187,  ...,     0,     0,     0],\n","        [  101,  2307,  3347,  ...,     0,     0,     0],\n","        [  101,  2428,  5632,  ...,     0,     0,     0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), tensor([0, 0, 0, 0, 1, 4, 1, 2, 0, 0, 0, 4, 0, 0, 0, 1, 0, 4, 0, 0, 1, 2, 0, 1,\n","        0, 5, 0, 0, 1, 0, 0, 0])]\n","[tensor([[  101,  1030,  1996,  ...,     0,     0,     0],\n","        [  101,  1030,  2120,  ...,     0,     0,     0],\n","        [  101,  1001,  5696,  ...,     0,     0,     0],\n","        ...,\n","        [  101,  1030, 11937,  ...,     0,     0,     0],\n","        [  101,  1030, 27034,  ...,     0,     0,     0],\n","        [  101,  2551,  2006,  ...,     0,     0,     0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"X3xrkbt_PiCA"},"source":["## "]},{"cell_type":"code","metadata":{"id":"XfPq47wIPgvV","executionInfo":{"status":"ok","timestamp":1602683193989,"user_tz":-420,"elapsed":428679,"user":{"displayName":"Duke Le","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR-JPf_nRsoniBlewD14UhrWIcpOFN4Z_lSChnyg=s64","userId":"09089920771448989471"}}},"source":[""],"execution_count":73,"outputs":[]}]}